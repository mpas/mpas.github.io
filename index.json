[{"categories":"[\"infrastructure\"]","content":"Requirements Colima installed When using Colima make sure to point to the correct docker socket using an export\nexport DOCKER_HOST=\u0026quot;unix://${HOME}/.colima/default/docker.sock K3D installed Helm installed Steps # start colima with 4 CPU and 8GB memory colima start --cpu 4 --memory 8 # add the correct Helm repositories echo \u0026#34;Adding Helm repos\u0026#34; \\ \u0026amp;\u0026amp; helm repo add rancher-latest https://releases.rancher.com/server-charts/latest \\ \u0026amp;\u0026amp; helm repo add rancher-stable https://releases.rancher.com/server-charts/stable \\ \u0026amp;\u0026amp; helm repo add jetstack https://charts.jetstack.io \\ \u0026amp;\u0026amp; helm repo update # create a cluster where rancher will be installed (name: rmaster -\u0026gt; this will become k3d-rmaster) ## - port 80/443 on localhost will be forwarded to 80/443 in the cluster/load balancer ## - the number of servers and agents are specified explicitly but they are the defaults! ## servers: the number of servers that will be run ## agents: the number of worker nodes k3d cluster create rmaster --port 80:80@loadbalancer --port 443:443@loadbalancer --servers 1 --agents 0 # change to the correct Kubernetes context ## actually not required as k3d will switch to the just created context kubectl config set-context k3d-rmaster # install Cert Manager kubectl create namespace cert-manager \\ \u0026amp;\u0026amp; helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --version v1.11.1 \\ --set installCRDs=true --debug # verify if cert-manager is running kubectl get pods --namespace cert-manager # install Rancher (this can take a while! So dont get scared when you see pods in error state!) ## - replicas: the number of rancher replicas ## - hostname: set the hostname to your correct ip; for this we are using https://nip.io ## an alternative could be using ngrok or manually update your hosts file! ## - bootstrapPassword: The password for login into the rancher web ui ## - global.cattle.psp.enabled: We disable pod security policies; we need to harden later on! kubectl create namespace cattle-system \\ \u0026amp;\u0026amp; helm install rancher rancher-latest/rancher \\ --namespace cattle-system \\ --set replicas=1 \\ --set hostname=192-168-68-107.nip.io \\ --set global.cattle.psp.enabled=false \\ --set bootstrapPassword=welcome123 \\ --debug # verify if rancher is running kubectl get pods --namespace cattle-system # create client clusters k3d cluster create rclient1 # (name: rclient1 -\u0026gt; this will become k3d-rclient1) k3d cluster create rclient2 # (name: rclient2 -\u0026gt; this will become k3d-rclient2) # import the client clusters via the web interface! ## make sure to pick the correct host; see the rancher install ## this will give an unsecure connection; no need to worry just proceed in your browser http://192-168-68-107.nip.io Resources GitHub Gist - Rancher Workshop GitHub Gist - Script to install rancher on your macOS machine ","date":"May 9, 2023","href":"https://mpas.github.io/posts/2023/05/09/20230509-running-rancher-on-k3s-using-k3d/","summary":"Requirements Colima installed When using Colima make sure to point to the correct docker socket using an export\nexport DOCKER_HOST=\u0026quot;unix://${HOME}/.colima/default/docker.sock K3D installed Helm installed Steps # start colima with 4 CPU and 8GB memory colima start --cpu 4 --memory 8 # add the correct Helm repositories echo \u0026#34;Adding Helm repos\u0026#34; \\ \u0026amp;\u0026amp; helm repo add rancher-latest https://releases.rancher.com/server-charts/latest \\ \u0026amp;\u0026amp; helm repo add rancher-stable https://releases.rancher.com/server-charts/stable \\ \u0026amp;\u0026amp; helm repo add jetstack https://charts.","tags":["kubernetes","rancher","k3d","docker"],"title":"Running Rancher on K3S using K3D","type":"posts"},{"categories":"[\"development\"]","content":"To quickly grab a link from an open Mac app and use this link into an org-mode document use the following package:\nGrab links from open Mac applications An optional keybind as follows:\n(add-hook \u0026#39;org-mode-hook (lambda () (define-key org-mode-map (kbd \u0026#34;C-c g\u0026#34;) \u0026#39;org-mac-grab-link))) This enables you to quickly grab a link using C-c g\n","date":"March 30, 2021","href":"https://mpas.github.io/posts/2021/03/30/20210330-capture-link-from-mac-app-into-org-mode-document/","summary":"To quickly grab a link from an open Mac app and use this link into an org-mode document use the following package:\nGrab links from open Mac applications An optional keybind as follows:\n(add-hook \u0026#39;org-mode-hook (lambda () (define-key org-mode-map (kbd \u0026#34;C-c g\u0026#34;) \u0026#39;org-mac-grab-link))) This enables you to quickly grab a link using C-c g","tags":["orgmode","emacs","elisp"],"title":"Capture link from Mac app into org-mode document","type":"posts"},{"categories":"[]","content":"Personally I really like using org-mode files for creating documentation and presentations. When working with screenshots my normal workflows always has been.\nCapture screenshot to clipboard Save the screenshot to a relative ./images folder and give it a descriptive name. Manually create link to the screenshot This works ok but after a while you get the feel that the whole process can be automated.\nThe following function can help in automating the whole process.\nCapture screenshot In your org-document paste using C-M-y (or call my/insert-clipboard-image) and automatically add the screenshot to an =./images- directory and insert the link to the image. ;; Overview ;; -------- ;; Inserts an image from the clipboard by prompting the user for a filename. ;; Default extension for the pasted filename is .png ;; A ./images directory will be created relative to the current Org-mode document to store the images. ;; The default name format of the pasted image is: ;; filename: \u0026lt;yyyymmdd\u0026gt;_\u0026lt;hhmmss\u0026gt;_-_\u0026lt;image-filename\u0026gt;.png ;; Important ;; -------- ;; This function depends on \u0026#39;pngpaste\u0026#39; to paste the clipboard image ;; -\u0026gt; $ brew install pngpaste ;; Basic Customization ;; ------------------- ;; Include the current Org-mode header as part of the image name. ;; (setq my/insert-clipboard-image-use-headername t) ;; filename: \u0026lt;yyyymmdd\u0026gt;_\u0026lt;hhmmss\u0026gt;_-_\u0026lt;headername\u0026gt;_-_\u0026lt;image-filename\u0026gt;.png ;; Include the buffername as part of the image name. ;; (setq my/insert-clipboard-image-use-buffername t) ;; filename: \u0026lt;yyyymmdd\u0026gt;_\u0026lt;hhmmss\u0026gt;_-_\u0026lt;buffername\u0026gt;_-_\u0026lt;image-filename\u0026gt;.png ;; Full name format ;; filename: \u0026lt;yyyymmdd\u0026gt;_\u0026lt;hhmmss\u0026gt;_-_\u0026lt;buffername\u0026gt;_-_\u0026lt;headername\u0026gt;_-_\u0026lt;image-filename\u0026gt;.png (defun my/insert-clipboard-image (filename) \u0026#34;Inserts an image from the clipboard\u0026#34; (interactive \u0026#34;sFilename to paste: \u0026#34;) (let ((file (concat (file-name-directory buffer-file-name) \u0026#34;images/\u0026#34; (format-time-string \u0026#34;%Y%m%d_%H%M%S_-_\u0026#34;) (if (bound-and-true-p my/insert-clipboard-image-use-buffername) (concat (s-replace \u0026#34;-\u0026#34; \u0026#34;_\u0026#34; (downcase (file-name-sans-extension (buffer-name)))) \u0026#34;_-_\u0026#34;) \u0026#34;\u0026#34;) (if (bound-and-true-p my/insert-clipboard-image-use-headername) (concat (s-replace \u0026#34; \u0026#34; \u0026#34;_\u0026#34; (downcase (nth 4 (org-heading-components)))) \u0026#34;_-_\u0026#34;) \u0026#34;\u0026#34;) filename \u0026#34;.png\u0026#34;))) ;; create images directory (unless (file-exists-p (file-name-directory file)) (make-directory (file-name-directory file))) ;; paste file from clipboard (shell-command (concat \u0026#34;pngpaste \u0026#34; file)) (insert (concat \u0026#34;[[./images/\u0026#34; (file-name-nondirectory file) \u0026#34;]]\u0026#34;)))) (map! :desc \u0026#34;Insert clipboard image\u0026#34; :n \u0026#34;C-M-y\u0026#34; \u0026#39;my/insert-clipboard-image) A nice setting in org-mode that also helps when viewing large screenshots is the following:\nThis display the taken screenshot in a acceptable format in your org-mode file.\n(after! org (setq org-image-actual-width (/ (display-pixel-width) 2))) ","date":"March 29, 2021","href":"https://mpas.github.io/posts/2021/03/29/20210329-paste-image-from-clipboard-directly-into-org-mode-document/","summary":"Personally I really like using org-mode files for creating documentation and presentations. When working with screenshots my normal workflows always has been.\nCapture screenshot to clipboard Save the screenshot to a relative ./images folder and give it a descriptive name. Manually create link to the screenshot This works ok but after a while you get the feel that the whole process can be automated.\nThe following function can help in automating the whole process.","tags":["emacs","orgmode","elisp","screenshot","image"],"title":"Paste image from clipboard directly into org-mode document","type":"posts"},{"categories":"[]","content":"Tracking time using Org Mode is simple and easy. You can quickly create reports of the time spend on specific tasks. But how do you aggregate time across tasks belonging to tags?\nThis can be achieved by using a simple formula and the usage of an awesome Org package called Org Aggregate.\nInput data The data below is used for time tracking, note that individual items are tagged!\n- Take out the trash :private: :LOGBOOK: CLOCK: [2021-03-12 Fri 11:24]--[2021-03-12 Fri 11:30] =\u0026gt; 0:06 :END: - Update document for client :client1: :LOGBOOK: CLOCK: [2021-03-12 Fri 12:45]--[2021-03-12 Fri 13:30] =\u0026gt; 0:45 :END: - Create my awesome note for work :work: :LOGBOOK: CLOCK: [2021-03-13 Sat 11:24]--[2021-03-13 Sat 12:53] =\u0026gt; 1:29 :END: - Fill in timesheet :work: :LOGBOOK: CLOCK: [2021-03-12 Fri 11:24]--[2021-03-12 Fri 11:40] =\u0026gt; 0:16 :END: Reporting #+BEGIN: clocktable :scope file :maxlevel 3 :tags t :match \u0026#34;work|client1\u0026#34; :header \u0026#34;#+TBLNAME: timetable\\n\u0026#34; #+TBLNAME: timetable | Tags | Headline | Time | | | T | |---------+---------------------------------------+------+------+------+-------| | | *Total time* | *2:30* | | | | |---------+---------------------------------------+------+------+------+-------| | | Report with filtered tags and sum... | 2:30 | | | | | | \\_ Input data | | 2:30 | | | | client1 | \\_ Update document for client | | | 0:45 | 00:45 | | work | \\_ Create my awesome note for work | | | 1:29 | 01:29 | | work | \\_ Fill in timesheet | | | 0:16 | 00:16 | #+TBLFM: $6=\u0026#39;(convert-org-clocktable-time-to-hhmm $5)::@1$6=\u0026#39;(format \u0026#34;%s\u0026#34; \u0026#34;T\u0026#34;) #+END: :tags t used to display the tags :match \u0026ldquo;work|client\u0026rsquo;= used to filter the tags of interest :header \u0026ldquo;#+TBLNAME: timetable\\n\u0026rdquo;= used to name our table so we can process it later on using Org Aggregate #+TBLFM: is using a function to correctly display time in hh:mm so we can use it later on to sum. Note: this is required as the package Org Aggregate that we are using to aggregate data is expecting the time in a hh:mm format 1 2 3 4 5 6 7 8 9 10 11 12 13 (defun convert-org-clocktable-time-to-hhmm (time-string) \u0026#34;Converts a time string to HH:MM\u0026#34; (if (\u0026gt; (length time-string) 0) (progn (let* ((s (s-replace \u0026#34;*\u0026#34; \u0026#34;\u0026#34; time-string)) (splits (split-string s \u0026#34;:\u0026#34;)) (hours (car splits)) (minutes (car (last splits))) ) (if (= (length hours) 1) (format \u0026#34;0%s:%s\u0026#34; hours minutes) (format \u0026#34;%s:%s\u0026#34; hours minutes)))) time-string)) Use Org Aggregate to sum the times of the tags\n#+BEGIN: aggregate :table \u0026#34;timetable\u0026#34; :cols \u0026#34;Tags sum(T);U\u0026#34; :cond (not (equal Tags \u0026#34;\u0026#34;)) #+TBLNAME: timetable | Tags | sum(T);U | |---------+----------| | client1 | 00:45 | | work | 01:45 | #+END: :cond used to filter empty rows from the data input! ","date":"March 16, 2021","href":"https://mpas.github.io/posts/2021/03/16/20210316-time-tracking-with-org-mode-and-sum-time-per-tag/","summary":"Tracking time using Org Mode is simple and easy. You can quickly create reports of the time spend on specific tasks. But how do you aggregate time across tasks belonging to tags?\nThis can be achieved by using a simple formula and the usage of an awesome Org package called Org Aggregate.\n","tags":["emacs","orgmode","timetracking"],"title":"Time tracking with Org Mode and sum time per tag","type":"posts"},{"categories":"[]","content":"When installing Doom Emacs and using org-mode the defaults bullets are `*`. In order to get some fancy bullets the following steps need to be taken.\nAdd the org-mode +pretty flag to your org settings in init.el To read more on the available flags check the org-mode Doom Emacs module `lang/org` :lang (org +pretty ) ; organize your plain life in plain text (setq org-superstar-headline-bullets-list \u0026#39;(\u0026#34;⁖\u0026#34; \u0026#34;◉\u0026#34; \u0026#34;○\u0026#34; \u0026#34;✸\u0026#34; \u0026#34;✿\u0026#34;) ) ","date":"October 16, 2020","href":"https://mpas.github.io/posts/2020/10/16/20201016-org-bullets-doom-emacs/","summary":"When installing Doom Emacs and using org-mode the defaults bullets are `*`. In order to get some fancy bullets the following steps need to be taken.\n","tags":["emacs","doom","orgmode"],"title":"Get pretty org-bullets in Doom Emacs","type":"posts"},{"categories":"[]","content":"I am software geek and a hands on developer/system architect. Interested in all kinds of software related technologies (languages, architectures and methodologies). Strong believer in the fact that one should keep knowledge up-to-date, but also understand why you should choose a particular technology for a given problem.\nKeeping knowledge up-to-date if fundamental and that is why regular visits to conferences like QCon, Devoxx, BuildStuf and various Meetup communities are part of my working habbit. Supporting the developer community by organising the yearly NextBuild Conference for Developers and by Developers.\nTech guy, wannabe cooking chef, traveller, likes to watch movies/series, beer enthusiast, tries to enjoy life.\nBelow are some place where you can find some more information about me:\nLinkedIn ~ Twitter ~ Flickr ~ StackOverflow ~ GitHub\nTechnical Experience During various projects for clients I had the opportunity to get acquainted with a variety of technologies and methodologies such as:\nGo, Elixir, DevOps, Consul, Continous Deployment/Delivery, Java, Groovy, Grails, Spock, SpringBoot, REST, Amazon Web Services, ElasticSearch, Fluentd, Kibana, MicroServices, Eureka, Spring, RabbitMQ, Amazon Webservices, Jenkins, Test Driven Development, Neo4J, MongoDB, JSON, Apache Tomcat, SonarQube, JMeter, IntelliJ, Nexus, Gradle, Maven, GIT, PostgreSQL, MySQL, Terraform, CloudFormation, Twitter BootStrap, Skeleton CSS, JavaScript, DevOps and lots more..\nResume You can find my resume online at http://mpas.github.io/resume\n","date":"October 6, 2020","href":"https://mpas.github.io/pages/about/","summary":"I am software geek and a hands on developer/system architect. Interested in all kinds of software related technologies (languages, architectures and methodologies). Strong believer in the fact that one should keep knowledge up-to-date, but also understand why you should choose a particular technology for a given problem.\nKeeping knowledge up-to-date if fundamental and that is why regular visits to conferences like QCon, Devoxx, BuildStuf and various Meetup communities are part of my working habbit.","tags":null,"title":"About","type":"pages"},{"categories":"[]","content":"You can find my resume online at http://mpas.github.io/resume\n","date":"October 6, 2020","href":"https://mpas.github.io/pages/resume/","summary":"You can find my resume online at http://mpas.github.io/resume","tags":null,"title":"Resume","type":"pages"},{"categories":"[]","content":"Ranger is a VIM-inspired filemanager for the console (https://ranger.github.io/) and can easily be installed by using brew install ranger. When working in the terminal sometimes it is nice to open the files in the default Finder app or use the excellent alternative called Path Finder. (https://cocoatech.com/#/)\nTo reveal your files in the Finder or Path Finder create a commands.py in ~/.config/ranger and paste the following code.\nfrom ranger.api.commands import Command class show_files_in_path_finder(Command): \u0026#34;\u0026#34;\u0026#34; :show_files_in_path_finder Present selected files in finder \u0026#34;\u0026#34;\u0026#34; def execute(self): import subprocess files = \u0026#34;,\u0026#34;.join([\u0026#39;\u0026#34;{0}\u0026#34; as POSIX file\u0026#39;.format(file.path) for file in self.fm.thistab.get_selection()]) reveal_script = \u0026#34;tell application \\\u0026#34;Path Finder\\\u0026#34; to reveal {{{0}}}\u0026#34;.format(files) activate_script = \u0026#34;tell application \\\u0026#34;Path Finder\\\u0026#34; to activate\u0026#34; script = \u0026#34;osascript -e \u0026#39;{0}\u0026#39; -e \u0026#39;{1}\u0026#39;\u0026#34;.format(reveal_script, activate_script) self.fm.notify(script) subprocess.check_output([\u0026#34;osascript\u0026#34;, \u0026#34;-e\u0026#34;, reveal_script, \u0026#34;-e\u0026#34;, activate_script]) class show_files_in_finder(Command): \u0026#34;\u0026#34;\u0026#34; :show_files_in_finder Present selected files in finder \u0026#34;\u0026#34;\u0026#34; def execute(self): import subprocess files = \u0026#34;,\u0026#34;.join([\u0026#39;\u0026#34;{0}\u0026#34; as POSIX file\u0026#39;.format(file.path) for file in self.fm.thistab.get_selection()]) reveal_script = \u0026#34;tell application \\\u0026#34;Finder\\\u0026#34; to reveal {{{0}}}\u0026#34;.format(files) activate_script = \u0026#34;tell application \\\u0026#34;Finder\\\u0026#34; to set frontmost to true\u0026#34; script = \u0026#34;osascript -e \u0026#39;{0}\u0026#39; -e \u0026#39;{1}\u0026#39;\u0026#34;.format(reveal_script, activate_script) self.fm.notify(script) subprocess.check_output([\u0026#34;osascript\u0026#34;, \u0026#34;-e\u0026#34;, reveal_script, \u0026#34;-e\u0026#34;, activate_script]) Restart Ranger and now you can execute the commands :show_files_in_pathfinder or :show_files_in_finder.\n","date":"November 28, 2018","href":"https://mpas.github.io/posts/2018/11/28/20181128-ranger-show-file-in-path-finder/","summary":"Ranger is a VIM-inspired filemanager for the console (https://ranger.github.io/) and can easily be installed by using brew install ranger. When working in the terminal sometimes it is nice to open the files in the default Finder app or use the excellent alternative called Path Finder. (https://cocoatech.com/#/)\nTo reveal your files in the Finder or Path Finder create a commands.py in ~/.config/ranger and paste the following code.\nfrom ranger.api.commands import Command class show_files_in_path_finder(Command): \u0026#34;\u0026#34;\u0026#34; :show_files_in_path_finder Present selected files in finder \u0026#34;\u0026#34;\u0026#34; def execute(self): import subprocess files = \u0026#34;,\u0026#34;.","tags":["ranger","pathfinder"],"title":"Ranger - Show File in Path Finder","type":"posts"},{"categories":"[]","content":"Last week I was lucky enough to host a talk during the Devoxx (UK) on the subject of \u0026ldquo;Infrastructure and System Monitoring using Prometheus\u0026rdquo;. You can find the used material here:\nSlides: https://speakerdeck.com/mpas/infrastructure-and-system-monitoring-using-prometheus Slides in PDF: https://drive.google.com/open?id=0Byx7lFSXlDU0TkJ1ZUZqdjU4MFE Code: https://github.com/mpas/infrastructure-and-system-monitoring-using-prometheus Feel free to share and distribute\n","date":"May 12, 2017","href":"https://mpas.github.io/posts/2017/05/12/20170512-infrastructure-and-system-monitoring-using-prometheus/","summary":"Last week I was lucky enough to host a talk during the Devoxx (UK) on the subject of \u0026ldquo;Infrastructure and System Monitoring using Prometheus\u0026rdquo;. You can find the used material here:\nSlides: https://speakerdeck.com/mpas/infrastructure-and-system-monitoring-using-prometheus Slides in PDF: https://drive.google.com/open?id=0Byx7lFSXlDU0TkJ1ZUZqdjU4MFE Code: https://github.com/mpas/infrastructure-and-system-monitoring-using-prometheus Feel free to share and distribute","tags":["infrastructure","monitoring","docker","prometheus","talks","devoxx"],"title":"Infrastructure and System Monitoring using Prometheus","type":"posts"},{"categories":"[]","content":"Ever wanted to create a Docker image that contains your Grails application? You are lucky! It is easy to do so..\nLet us first create a new Grails application. In the example we will create a basic application using the rest-profile.\nPrerequisite : Docker is installed on your machine.\n$ grails create-app docker-test --profile rest-api After the Grails application has been created, we will need to add the following files to our project.\nDockerfile (determines what our Docker image will contain) docker-entrypoint.sh (script that is responsible for starting our Grails application) // file: /src/main/docker/Dockerfile FROM openjdk:latest # set environment options ENV JAVA_OPTS=\u0026#34;-Xms64m -Xmx256m -XX:MaxMetaspaceSize=128m\u0026#34; RUN mkdir -p /app WORKDIR /app COPY /app/application.jar application.jar COPY /app/docker-entrypoint.sh docker-entrypoint.sh # Set file permissions RUN chmod +x /app/docker-entrypoint.sh # Set start script as enrypoint ENTRYPOINT [\u0026#34;./docker-entrypoint.sh\u0026#34;] // file: /src/main/docker/app/docker-entrypoint.sh #!/bin/bash set -e exec java ${JAVA_OPTS} -jar application.jar $@ # exec java ${JAVA_OPTS} -Dserver.port=${SERVER_PORT} -jar application.jar $@ Next step is to add the tasks to our build.gradle so it can generate the Docker image.\nSo add the following snippet to your build.gradle file!\n// file: /build.gradle String getDockerImageName() { \u0026#34;docker-test\u0026#34; } task buildDockerImage(type:Exec) { group = \u0026#39;docker\u0026#39; description = \u0026#39;Build a docker image\u0026#39; commandLine \u0026#39;docker\u0026#39;, \u0026#39;build\u0026#39;, \u0026#39;-f\u0026#39;, \u0026#39;build/docker/Dockerfile\u0026#39;, \u0026#39;-t\u0026#39;, \u0026#34;${dockerImageName}\u0026#34;, \u0026#39;build/docker\u0026#39; doFirst { println \u0026#34;\u0026gt;\u0026gt; Creating image: ${dockerImageName}\u0026#34; /* copy the generate war file to /build/docker/app */ copy { from war.archivePath into \u0026#39;build/docker/app/\u0026#39; } /* copy artifacts from src/main/docker/app into the build/docker/app */ copy { from \u0026#39;src/main/docker/app/\u0026#39; into \u0026#39;build/docker/app\u0026#39; } /* copy Dockerfile from src/main/docker into the build/docker */ copy { from(\u0026#39;src/main/docker/\u0026#39;) { include \u0026#39;Dockerfile\u0026#39; } into \u0026#39;build/docker\u0026#39; } /* rename war file to application.jar */ file(\u0026#34;build/docker/app/${war.archiveName}\u0026#34;).renameTo(\u0026#34;build/docker/app/application.jar\u0026#34;) } } Now that we have everyting in place we can build the image and start the container,\nCreate the docker image using assemble target and buildDockerImage\n$ ./gradlew assemble buildDockerImage Run a container based on the previous created image\n$ docker run -it --rm -p 8080:8080 docker-test This will run the container in interactive mode (-it) and the container will be removed when we stop the container (--rm). Port 8080 in the container will be available on port 8080 on your host system (-p 8080:8080).\nThis will run the specified container and the endpoint will be available using your browser. Just visit http://localhost:8080\n","date":"November 25, 2016","href":"https://mpas.github.io/posts/2016/11/25/20161125-dockerize-your-grails-application/","summary":"Ever wanted to create a Docker image that contains your Grails application? You are lucky! It is easy to do so..\nLet us first create a new Grails application. In the example we will create a basic application using the rest-profile.\nPrerequisite : Docker is installed on your machine.\n$ grails create-app docker-test --profile rest-api After the Grails application has been created, we will need to add the following files to our project.","tags":["grails","docker"],"title":"Dockerize your Grails application","type":"posts"},{"categories":"[]","content":"In our current project we where heavily focussed on the usage of Gradle plugins to create Docker images. We used plugins to create the images and push them to our AWS ECR repositories. When using these plugins we hit various bugs related to the fact that not all developers where using Linux operating systems to test our their containers. At the end we took a look on how we could create those images without using additional plugins.\nPrerequisite : Docker is installed on your machine.\nCreating an image The following snippet will create a Docker image using the task gradle buildDockerImage\n- application layout | build.gradle | \u0026gt; src | \u0026gt;- main | \u0026gt;-- docker (contains a Dockerfile) | \u0026gt;--- app (contains data that can be used in your Dockerfile) /* You can put some logic in the getDockerImageName to determine how your Docker image should be created. */ String getDockerImageName() { \u0026#34;my-first-docker-image\u0026#34; } /* Execute a docker build using commandline pointing to our Dockerfile that has been copied to /build/docker/. */ task buildDockerImage(type:Exec) { group = \u0026#39;docker\u0026#39; description = \u0026#39;Build a docker image\u0026#39; commandLine \u0026#39;docker\u0026#39;, \u0026#39;build\u0026#39;, \u0026#39;-f\u0026#39;, \u0026#39;build/docker/Dockerfile\u0026#39;, \u0026#39;-t\u0026#39;, \u0026#34;${dockerImageName}\u0026#34;, \u0026#39;build/docker\u0026#39; doFirst { println \u0026#34;\u0026gt;\u0026gt; Creating image: ${dockerImageName}\u0026#34; /* copy artifacts from src/main/docker/app into the build/docker/app */ copy { from \u0026#39;src/main/docker/app/\u0026#39; into \u0026#39;build/docker/app\u0026#39; } /* copy Dockerfile from src/main/docker into the build/docker */ copy { from(\u0026#39;src/main/docker/\u0026#39;) { include \u0026#39;Dockerfile\u0026#39; } into \u0026#39;build/docker\u0026#39; } } } pushing an image Pushing an image without using plugins is just as easy.\npushDockerImage(type: Exec) { group = \u0026#39;docker\u0026#39; description = \u0026#39;Push a docker image\u0026#39; commandLine \u0026#39;docker\u0026#39;, \u0026#39;push\u0026#39;, \u0026#34;${dockerImageName}\u0026#34; doFirst { println \u0026#34;\u0026gt;\u0026gt; Pushing image: ${dockerImageName}\u0026#34; } } Using this approach without using unneeded Gradle plugins resulted in a an easy way to create containers on different platforms.\n","date":"November 24, 2016","href":"https://mpas.github.io/posts/2016/11/24/20161124-creating-pushing-docker-images-using-gradle-without-plugins/","summary":"In our current project we where heavily focussed on the usage of Gradle plugins to create Docker images. We used plugins to create the images and push them to our AWS ECR repositories. When using these plugins we hit various bugs related to the fact that not all developers where using Linux operating systems to test our their containers. At the end we took a look on how we could create those images without using additional plugins.","tags":["docker","gradle","groovy"],"title":"Creating/Pushing Docker images using Gradle without plugins","type":"posts"},{"categories":"[]","content":"Prometheus is a an open-source monitoring system with a dimensional data model, flexible query language, efficient time series database and modern alerting approach.\nMetric data is pulled (on a regular time-interval) from so called exporters which expose the metrics coming from applications/operating systems etc..\n+------------------+ +----------+ Visualize data | +------------+ | | Grafana +---\u0026gt; coming from | | Dockerized | | +----+-----+ Prometheus | | Application| | | | +------------+ | ^ | +------------+ | Pull data +----------+ | | | CAdvisor +---------\u0026gt;-------+Prometheus+----------+ | +------------+ | +---------++ | | | | Operating System | | | with | | | Docker installed | | | | v +------------------+ Prometheus collects data coming from remote systems In the diagram above cAdvisor is a so called exporter. There are other exporters like e.g. Node Exporter that exposes machine metrics. cAdvisor is used to get Docker container metrics.\ncAdvisor cAdvisor is a project coming from Google and analyzes resource usage and performance characteristics of running Docker containers! When running a Dockerized application and starting a cAdvisor container you will have instant metrics available for all running containers.\nA lot of metrics are exposed by cAdvisor of which one is the metric container_last_seen. You can use this metric in Prometheus to identify if a container has left the building :) The challenge with Prometheus is that it keeps the data for a specific amount of time the so called Stale Timeout. This means that Prometheus will keep reporting back that the data has been received until this timeout has occurred (default 5 minutes). This is of course too much if we need to identify if a container has gone.\nSo if you would normally query like this:\ncount(container_last_seen{job=\u0026#34;\u0026lt;jobname\u0026gt;\u0026#34;,name=~\u0026#34;.*\u0026lt;containername\u0026gt;.*\u0026#34;}) This would get results until 5 minutes.. way to far\u0026hellip;\nA simple alternate query to identify if the container has gone is like below:\ncount(time() - container_last_seen{job=\u0026#34;\u0026lt;jobname\u0026gt;\u0026#34;,name=~\u0026#34;.*\u0026lt;containername\u0026gt;.*\u0026#34;} \u0026lt; 30) OR vector(0) The \u0026lsquo;30\u0026rsquo; is the time in seconds before we want to be notified if a container has gone. This time has to be larger then the pull interval for your job.\nWhen using the mentioned query you can create a nice Singlestat panel in Grafan so you can display an alert when the container is gone.\n","date":"November 17, 2016","href":"https://mpas.github.io/posts/2016/11/17/20161117-identifying-docker-container-outage-using-prometheus/","summary":"Prometheus is a an open-source monitoring system with a dimensional data model, flexible query language, efficient time series database and modern alerting approach.\nMetric data is pulled (on a regular time-interval) from so called exporters which expose the metrics coming from applications/operating systems etc..\n+------------------+ +----------+ Visualize data | +------------+ | | Grafana +---\u0026gt; coming from | | Dockerized | | +----+-----+ Prometheus | | Application| | | | +------------+ | ^ | +------------+ | Pull data +----------+ | | | CAdvisor +---------\u0026gt;-------+Prometheus+----------+ | +------------+ | +---------++ | | | | Operating System | | | with | | | Docker installed | | | | v +------------------+ Prometheus collects data coming from remote systems In the diagram above cAdvisor is a so called exporter.","tags":["prometheus","docker","cadvisor"],"title":"Identifying Docker container outage using Prometheus","type":"posts"},{"categories":"[]","content":" Service Discovery Health Checking Key/Value Store Multi Datacenter For more information on Consul itself please have a look in the excellent documentation.\nConsul Intro Consul documentation Is it really easy? Setting up a Consul cluster seems easy, just follow the many tutorials out there and you will have a Consul cluster running in a few steps on your local machine\u0026hellip;\nBut hey.. what if you need to deploy this cluster on an AWS environment? How do you create the cluster and how can you make sure it is always available?\nThis simple write up is just an example to give you an idea how this Consul cluster can be created and provisioned by using Terraform only. The goal is to have a cluster using the official Docker images provided by Consul itself running on EC2 nodes.\nCreating a Consul cluster The principle is not that hard\u0026hellip; Consul nodes can discover each other based on IP Address. If you feed the Consul cluster members with IP Addresses that are part of the cluster you are good to go. In the example case we are going to start a number of Consul cluster members. The first node will be unable to form a cluster but if the second node starts up it will get the ip from the first node and the the first node will then know the ip of the second one.. etc.. So if you start up more than 2 nodes you will be good to go.\n+------+ +------+ +------+ +------+ +------+ |Consul| |Consul| |Consul| |Consul| |Consul| |Node 1| |Node 2| |Node 3| |Node 4| |Node n| +------+ +------+ +------+ +------+ +------+ \u0026lt; Find each other based on ip address \u0026gt; The power is in the user-data script that is used for bootstrapping the Consul cluster nodes. In the example case they will find each other based on a query using aws ec2 describe-instances that will find nodes with a specific name, and from those identified nodes we will extract the IP Addresses that will be used to joint the Consul cluster. You can always modify the query to your own needs off course. The user-data script is used in the launch configuration.\nSo enough talk\u0026hellip; lets start :)\nThe given examples are intentionally kept simple!! So you will need to tweak your Terraform code according to your needs\nStep 1: Define a launch Configuration The core component of our Consul cluster is the launch configuration. This launch configuration determines what user-data file needs to be executed when launching a new instance.\nresource \u0026#34;aws_launch_configuration\u0026#34; \u0026#34;consul-cluster\u0026#34; { /* in this case we use a docker optimized ami, because we are going to use the official Consul docker image as a starter. You can always use an ami on which you install docker manually! */ image_id = \u0026#34;docker-optimized-ami-0123456789\u0026#34; /* the user-data script that holds the magic */ user_data = \u0026#34;${data.template_file.consul-cluster.rendered}\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; /* make sure you open the correct ports so the Consul nodes can discover each other the actual security group is not shown */ security_groups = [\u0026#34;sg-0123456789\u0026#34;] key_name = your-deploy-key /* us a policy which grants read access to the EC2 api */ iam_instance_profile = \u0026#34;arn:aws:iam::0123456789:read_ec2_policy/ec2\u0026#34; } /* The template file used for the user-data */ data \u0026#34;template_file\u0026#34; \u0026#34;consul-cluster\u0026#34; { template = \u0026#34;user-data-consul-cluster.tpl\u0026#34;)}\u0026#34; vars { // the name must match the Name tag of the autoscaling group consul_cluster_name = \u0026#34;consul-cluster-member\u0026#34; // the number of instances that need to be in the cluster to be healthy consul_cluster_min_size = 3 } } Step 2: Create the template file The user-data file is going to query AWS using the aws describe-instances api and will return ec2 nodes that have a matching name using the --filters option. 'Name=tag:Name,Values=${consul_cluster_name}'\nAll the retrieved instances are then queried for their private ip and the values are stored in a list. After completing the list the instance ip for the current machine is removed.\nA Consul specific join string is composed and provided to the docker image. This enables the Consul docker image to check for available servers when starting.\n// File: user-data-consul-cluster.tpl #!/bin/bash -ex exec \u0026gt; \u0026gt;(tee /var/log/user-data.log|logger -t user-data -s 2\u0026gt;/dev/console) 2\u0026gt;\u0026amp;1 echo \u0026#39;installing additional software\u0026#39; for i in {1..5} do yum install -y aws-cli \u0026amp;\u0026amp; break || sleep 120 done ################################################################################ # Running Consul ################################################################################ # get current instance ip INSTANCE_IP=$(curl http://169.254.169.254/latest/meta-data/local-ipv4) # get list of available Consul servers; based on Name (value) tag! IP_LIST=$(aws ec2 describe-instances --region us-east-1 \\ --filters \u0026#39;Name=tag:Name,Values=${consul_cluster_name}\u0026#39; \\ \u0026#39;Name=instance-state-name,Values=running\u0026#39; \\ --query \u0026#34;Reservations[*].Instances[*].PrivateIpAddress\u0026#34; \\ --output=text) # remove the current instance ip from the list of available servers IP_LIST=\u0026#34;$${IP_LIST/$$INSTANCE_IP/}\u0026#34; # remove duplicated spaces, \\r\\n and replace space by \u0026#39;,\u0026#39; IP_LIST=$(echo $$IP_LIST | tr -s \u0026#34; \u0026#34; | tr -d \u0026#39;\\r\\n\u0026#39; | tr -s \u0026#39; \u0026#39; \u0026#39;,\u0026#39;) # create join string for i in $(echo $IP_LIST | sed \u0026#34;s/,/ /g\u0026#34;) do JOIN_STRING+=\u0026#34;-retry-join $i \u0026#34; done # - run Consul docker run -d --net=host \\ -e \u0026#39;CONSUL_LOCAL_CONFIG={\u0026#34;skip_leave_on_interrupt\u0026#34;: true}\u0026#39; \\ --name consul-server consul:latest \\ agent -server -bind=$INSTANCE_IP $JOIN_STRING \\ -bootstrap-expect=${consul_cluster_min_size} -ui -client 0.0.0.0 # ------------------------------------------------------------------------------ Step 3: Create an autoscaling group /* creates an autoscaling group so servers are created when needed */ resource \u0026#34;aws_autoscaling_group\u0026#34; \u0026#34;consul-cluster\u0026#34; { min_size = 3 max_size = 5 desired_capacity = 3 min_elb_capacity = 3 launch_configuration = \u0026#34;${aws_launch_configuration.consul-cluster.name}\u0026#34; load_balancers = [\u0026#34;${aws_elb.consul.id}\u0026#34;] tag { key = \u0026#34;Name\u0026#34; /* note: this is the value that is being searched for in the user-data */ value = \u0026#34;consul-cluster-member\u0026#34; propagate_at_launch = true } } Step 4: Create an ELB as frontend for the Consul cluster resource \u0026#34;aws_elb\u0026#34; \u0026#34;consul-cluster\u0026#34; { name = \u0026#34;consul-cluster\u0026#34; subnets = [\u0026#34;sn-0123456789\u0026#34;] security_groups = [\u0026#34;sg-0123456789\u0026#34;] listener { instance_port = 8300 instance_protocol = \u0026#34;tcp\u0026#34; lb_port = 8300 lb_protocol = \u0026#34;tcp\u0026#34; } listener { instance_port = 8301 instance_protocol = \u0026#34;tcp\u0026#34; lb_port = 8301 lb_protocol = \u0026#34;tcp\u0026#34; } listener { instance_port = 8302 instance_protocol = \u0026#34;tcp\u0026#34; lb_port = 8302 lb_protocol = \u0026#34;tcp\u0026#34; } listener { instance_port = 8400 instance_protocol = \u0026#34;tcp\u0026#34; lb_port = 8400 lb_protocol = \u0026#34;tcp\u0026#34; } listener { instance_port = 8500 instance_protocol = \u0026#34;http\u0026#34; lb_port = 8500 lb_protocol = \u0026#34;http\u0026#34; } listener { instance_port = 8600 instance_protocol = \u0026#34;tcp\u0026#34; lb_port = 8600 lb_protocol = \u0026#34;tcp\u0026#34; } health_check { target = \u0026#34;HTTP:8500/v1/status/leader\u0026#34; healthy_threshold = 2 unhealthy_threshold = 2 interval = 30 timeout = 5 } } When putting all the pieces together you should now have a running Consul cluster!\n","date":"November 16, 2016","href":"https://mpas.github.io/posts/2016/11/16/20161116-building-a-consul-cluster-using-terraform-aws/","summary":"Service Discovery Health Checking Key/Value Store Multi Datacenter For more information on Consul itself please have a look in the excellent documentation.\nConsul Intro Consul documentation Is it really easy? Setting up a Consul cluster seems easy, just follow the many tutorials out there and you will have a Consul cluster running in a few steps on your local machine\u0026hellip;\nBut hey.. what if you need to deploy this cluster on an AWS environment?","tags":["consul","terraform","aws"],"title":"Building a Consul cluster using Terraform/AWS","type":"posts"},{"categories":"[]","content":"When using actuator endpoints to expose metrics in a Grails (Spring Boot) application, it may be useful to run the metrics on a different port.\nThis enables you to hide the metrics for the public and use the different port in an AWS infrastucture so that the metrics are only available internal.\nLet us first enable the actuator endpoints\n// File: grails-app/conf/application.yml # Spring Actuator Endpoints are Disabled by Default endpoints: enabled: true jmx: enabled: true Change the port on which the metrics runs, add the lines below to the appl\n// File: grails-app/conf/application.yml management: port: 9000 Now when you start your Grails application it run on port 8080 and the metrics are available on port 9090/metrics\n","date":"November 2, 2016","href":"https://mpas.github.io/posts/2016/11/02/20161101-change-the-port-of-actuator-endpoint/","summary":"When using actuator endpoints to expose metrics in a Grails (Spring Boot) application, it may be useful to run the metrics on a different port.\nThis enables you to hide the metrics for the public and use the different port in an AWS infrastucture so that the metrics are only available internal.\nLet us first enable the actuator endpoints\n// File: grails-app/conf/application.yml # Spring Actuator Endpoints are Disabled by Default endpoints: enabled: true jmx: enabled: true Change the port on which the metrics runs, add the lines below to the appl","tags":["grails","springboot"],"title":"Change the port of actuator endpoint in a Grails application","type":"posts"},{"categories":"[]","content":"Ever had a need to access something from within a Docker container that runs on the host system?\nWhen using native Docker on OSX you have bad luck. When configuring a container and pointing that to localhost will result in the fact the your software will be targeted at the localhost of the docker container.\nA solution for this isto define a new local loopback to your localhost\n$ sudo ifconfig lo0 alias 172.16.123.1 This will define a loopback network interface that points to your localhost. When you need to access the localhost you can use this ip.\n","date":"November 1, 2016","href":"https://mpas.github.io/posts/2016/11/01/20161101-access-localhost-using-native-docker-for-mac/","summary":"Ever had a need to access something from within a Docker container that runs on the host system?\nWhen using native Docker on OSX you have bad luck. When configuring a container and pointing that to localhost will result in the fact the your software will be targeted at the localhost of the docker container.\nA solution for this isto define a new local loopback to your localhost\n$ sudo ifconfig lo0 alias 172.","tags":["docker","osx","mac"],"title":"Accessing localhost from a Docker Container using native Docker for Mac","type":"posts"},{"categories":"[]","content":"Getting exposed to Amazon Web Services is fun! Certainly when you see that the infrastructure is growing and supporting the daily need of developers and the business. You slowly start adding services and try to keep everything in a state so that it is repeatable and maintainable. At a certain moment it becomes clear that you need the concept of Infrastructure As Code.\nThe Amazon way of doing this is by using AWS CloudFormation. This enables you to define the infrastructure in a JSON/YAML format and apply the changes to the infrastructure.\nOur team manages a bunch of environments using services like AWS ECS, EC2, ElasticSearch, RDS and more.. Maintaining this infrastructure in CloudFormation seemed the standard way of doing things until we started a proof-of-concept with Terraform.\nWhy did we start this proof-of-concept?? Mainly because the overwhelming pieces of code that we needed to maintain in CloudFormation became unmaintainable. The use of Terraform was so successful that we decide to rewrite our entire infrastructure codebase using Terraform.\nThe advantages when using Terraform are:\nless code to maintain because Terraform is less verbose when using Terraform an infrastructure change can be planned, this shows what is going to be changed before actually executing the change $ terraform plan See what the changes are and then when everything seems ok\u0026hellip;\n$ terraform apply Currently we have our entire Infrastructure in Terraform and we could never be more happier. Terraform came to our rescue!\n","date":"October 9, 2016","href":"https://mpas.github.io/posts/2016/10/09/20161009-terraform-to-the-rescue/","summary":"Getting exposed to Amazon Web Services is fun! Certainly when you see that the infrastructure is growing and supporting the daily need of developers and the business. You slowly start adding services and try to keep everything in a state so that it is repeatable and maintainable. At a certain moment it becomes clear that you need the concept of Infrastructure As Code.\nThe Amazon way of doing this is by using AWS CloudFormation.","tags":["devops","aws","terraform"],"title":"Terraform to the rescue","type":"posts"},{"categories":"[]","content":"Functional Rest API testing with Grails is easy and fun. We will be creating a simple Rest Controller and test it using Spock and Rest Client Builder.\nWhen running the functional test a real container will be started on a specific port and tests are run against the running container. Just as it should be.\nScenario: Performing a GET request on a url (http://localhost:8080/helloworld) should return a HTTP Status 200 and data with a json payload\n{\u0026#34;message\u0026#34;:\u0026#34;helloworld\u0026#34;} So lets get started!\nCreate a Grails application\n$ grails create-app RestHelloWorld Update your build.gradle to include the Rest Client Builder dependencies which we will need later on\ndependencies { // add the following line to the \u0026#39;dependencies\u0026#39; section testCompile \u0026#34;org.grails:grails-datastore-rest-client:4.0.7.RELEASE\u0026#34; } Create an Integration Test\n$ grails create-integration-test HelloWorld Create a test method inside the integration test\nOpen up the created HelloWorldControllerSpec inside the /src/integration-test/groovy/resthelloworld/ package\npackage resthelloworld import grails.test.mixin.integration.Integration import grails.transaction.* import spock.lang.* import grails.plugins.rest.client.RestBuilder import grails.plugins.rest.client.RestResponse @Integration @Rollback class HelloWorldSpec extends Specification { def setup() { } def cleanup() { } def \u0026#34;Ask for a nice HelloWorld\u0026#34;() { given: RestBuilder rest = new RestBuilder() when: RestResponse response = rest.get(\u0026#34;http://localhost:8080/helloworld/\u0026#34;) then: response.status == 200 and: response.json.message == \u0026#34;helloworld\u0026#34; } } Run your test\n$ grails test-app Offcourse this will fail as we do not have implement the controller yet.\nCreate a Rest controller\n$ cd RestHelloWorld $ grails create-controller HelloWorld Note: The generation of the controller also create a Unit Test for the controller, default this test will fail. We are going to delete the generated Unit Test as we do not need it now. This test is located under the /src/test/ groovy package.\n$ rm ./src/test/groovy/resthelloworld/HelloWorldControllerSpec.groovy Implement the controller function that will return data\npackage resthelloworld class HelloWorldController { def index() { render(status: 200, contentType: \u0026#34;application/json\u0026#34;) { [\u0026#34;message\u0026#34; : \u0026#34;helloworld\u0026#34;] } } } Modify UrlMapping\nIn order to get our newly generated controller accessible via Rest we need to modify our UrlMappings.\nclass UrlMappings { static mappings = { \u0026#34;/$controller/$action?/$id?(.$format)?\u0026#34;{ constraints { // apply constraints here } } \u0026#34;/\u0026#34;(view:\u0026#34;/index\u0026#34;) \u0026#34;500\u0026#34;(view:\u0026#39;/error\u0026#39;) \u0026#34;404\u0026#34;(view:\u0026#39;/notFound\u0026#39;) // add the line below \u0026#34;/helloworld/\u0026#34; (controller: \u0026#34;helloWorld\u0026#34;, action: \u0026#34;index\u0026#34;, method: \u0026#34;GET\u0026#34;) } } Test your app\n$ grails test-app You should find that your tests are fine now :)\n$ grails test-app BUILD SUCCESSFUL Total time: 2.054 secs | Tests PASSED ","date":"November 19, 2015","href":"https://mpas.github.io/posts/2015/11/19/20151119-functional-rest-api-testing/","summary":"Functional Rest API testing with Grails is easy and fun. We will be creating a simple Rest Controller and test it using Spock and Rest Client Builder.\nWhen running the functional test a real container will be started on a specific port and tests are run against the running container. Just as it should be.\nScenario: Performing a GET request on a url (http://localhost:8080/helloworld) should return a HTTP Status 200 and data with a json payload","tags":["grails","spock","rest","testing"],"title":"Functional Rest API Testing with Grails/Rest Client Builder","type":"posts"},{"categories":"[]","content":"If you find yourself into a situation where you have a need for non Microsoft mail client that needs support for Microsoft Exchange then you are often out of luck. In my case I needed Exchange support for the terrific PostBox mail client.\nAs for now PostBox does not support Microsoft Exhange natively so the hunt starts on how to get Exchange working. As it stands most companies also enable Exchange Web Access (or Outlook Web Access [OWA]) so maybe we can use that to feed our native mail client.\nEnter the use of DavMail!\nDavmail Gateway Davmail is a local mail proxy that can work together with Microsoft Exchange [OWA] in a way that DavMail is actually connecting to a Exchange OWA and your mail client connects to DavMail as a proxy.\nConfigure Davmail In order to get DavMail working correctly you need to provide the correct settings so it can use the OWA endpoint.\nConfigure PostBox In order to get PostBox working with DavMail you need to create an outgoing mail server and an account that will use that outgoing mailserver.\nConfigure PostBox - Outgoing mailserver Configure PostBox - Account setup Configure PostBox - Identity setup Now you are ready to send mail using your PostBox Client using DavMail and OWA.\n","date":"November 17, 2015","href":"https://mpas.github.io/posts/2015/11/17/20151117-davmail/","summary":"If you find yourself into a situation where you have a need for non Microsoft mail client that needs support for Microsoft Exchange then you are often out of luck. In my case I needed Exchange support for the terrific PostBox mail client.\nAs for now PostBox does not support Microsoft Exhange natively so the hunt starts on how to get Exchange working. As it stands most companies also enable Exchange Web Access (or Outlook Web Access [OWA]) so maybe we can use that to feed our native mail client.","tags":["davmail","exchange","owa","postbox"],"title":"Using DavMail Gateway as a mail proxy for Microsoft Exchange (OWA)","type":"posts"},{"categories":"[]","content":"Spock is a nice framework to execute integration tests in your Grails application. It may happen that the Spock test actually creates some domain objects and you want to clean them out on everuy single run of your feature test methods.\nSpock provides a setup() and cleanup() method.\nWhen you want to remove your domain objects after each feature test has run you can execute the following:\ndef setup() { ... } def cleanup() { // make sure to clear out the database on after test \u0026lt;YourDomainObject\u0026gt;.withNewSession { \u0026lt;YourDomainObject\u0026gt;.findAll().each { it.delete(flush: true) } } } We need the .withNewSession because there is no Hibernate session provided in the setup() and cleanup() methods.\n","date":"October 30, 2015","href":"https://mpas.github.io/posts/2015/10/30/20151030-grails-spock-clean-domain/","summary":"Spock is a nice framework to execute integration tests in your Grails application. It may happen that the Spock test actually creates some domain objects and you want to clean them out on everuy single run of your feature test methods.\nSpock provides a setup() and cleanup() method.\nWhen you want to remove your domain objects after each feature test has run you can execute the following:\ndef setup() { .","tags":["grails","spock"],"title":"Cleaning Grails Domain Objects in a Spock Test","type":"posts"},{"categories":"[]","content":"Adding support for WebSockets / Stomp in a Spring Boot application has never been more easy. You can use WebSockets to receive serverside events or push data to the server using WebSockets.\nThe following example will enable a server to send messages to a WebSocket/Stomp client.\nModify build.gradle dependencies { compile(\u0026#34;org.springframework.boot:spring-boot-starter-web\u0026#34;) compile(\u0026#34;org.springframework.boot:spring-boot-starter-websocket\u0026#34;) compile(\u0026#34;org.springframework:spring-messaging\u0026#34;) testCompile(\u0026#34;junit:junit\u0026#34;) } Create a WebSocket configuration class that holds the configuration @Configuration @EnableWebSocketMessageBroker public class WebSocketConfig extends AbstractWebSocketMessageBrokerConfigurer { @Override public void registerStompEndpoints(StompEndpointRegistry registry) { // the endpoint for websocket connections registry.addEndpoint(\u0026#34;/stomp\u0026#34;).withSockJS(); } @Override public void configureMessageBroker(MessageBrokerRegistry config) { // use the /topic prefix for outgoing WebSocket communication config.enableSimpleBroker(\u0026#34;/topic\u0026#34;); // use the /app prefix for others config.setApplicationDestinationPrefixes(\u0026#34;/app\u0026#34;); } } Now a client that connects to /stomp endpoint is able to receive WebSocket messages.\nCreate a service that is going to send the data to a WebSocket endpoint @Service public class ScheduleTask { @Autowired private SimpMessagingTemplate template; // this will send a message to an endpoint on which a client can subscribe @Scheduled(fixedRate = 5000) public void trigger() { // sends the message to /topic/message this.template.convertAndSend(\u0026#34;/topic/message\u0026#34;, \u0026#34;Date: \u0026#34; + new Date()); } } Create a client that is able to receive the message \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;WebSocket Stomp Receiving Example\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div\u0026gt; \u0026lt;h3\u0026gt;Messages:\u0026lt;/h3\u0026gt; \u0026lt;ol id=\u0026#34;messages\u0026#34;\u0026gt;\u0026lt;/ol\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;//cdn.jsdelivr.net/jquery/1.11.2/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;//cdn.jsdelivr.net/sockjs/0.3.4/sockjs.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;//cdnjs.cloudflare.com/ajax/libs/stomp.js/2.3.3/stomp.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; $(document).ready(function() { var messageList = $(\u0026#34;#messages\u0026#34;); // defined a connection to a new socket endpoint var socket = new SockJS(\u0026#39;/stomp\u0026#39;); var stompClient = Stomp.over(socket); stompClient.connect({ }, function(frame) { // subscribe to the /topic/message endpoint stompClient.subscribe(\u0026#34;/topic/message\u0026#34;, function(data) { var message = data.body; messageList.append(\u0026#34;\u0026lt;li\u0026gt;\u0026#34; + message + \u0026#34;\u0026lt;/li\u0026gt;\u0026#34;); }); }); }); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; The whole example project can be downloaded https://github.com/mpas/spring-boot-websocket-stomp-server-send-example\n","date":"June 16, 2015","href":"https://mpas.github.io/posts/2015/06/16/20150616-springboot-websocket-support/","summary":"Adding support for WebSockets / Stomp in a Spring Boot application has never been more easy. You can use WebSockets to receive serverside events or push data to the server using WebSockets.\nThe following example will enable a server to send messages to a WebSocket/Stomp client.\n","tags":["spring-boot","websocket"],"title":"Adding WebSocket/Stomp support to a Spring Boot application","type":"posts"},{"categories":"[]","content":"When creating a Grails WAR/JAR file using:\n$ grails war The resulting artifact can be run in production mode using:\n$ java -Dgrails.env=prod -Dserver.port=9000 -jar \u0026lt;name-of-jar-file\u0026gt;.jar ","date":"June 11, 2015","href":"https://mpas.github.io/posts/2015/06/11/20150611-run-grails-war-in-production-mode/","summary":"When creating a Grails WAR/JAR file using:\n$ grails war The resulting artifact can be run in production mode using:\n$ java -Dgrails.env=prod -Dserver.port=9000 -jar \u0026lt;name-of-jar-file\u0026gt;.jar ","tags":["grails"],"title":"Run a Grails 3 generated Fat Jar file in production mode","type":"posts"},{"categories":"[]","content":"When creating an Docker image it is nice to have predefined users and vhosts without manually having to create them after the Docker RabbitMQ image has started.\nThe following is a Dockerfile that extends the default Docker RabbitMQ image including the Management Plugin and it creates a standard set of users / vhosts when the container is created from the image.\nIt involves a init.sh script that is used to create the users and vhosts.\nThe Docker File\nFROM rabbitmq:3-management # Add script to create default users / vhosts ADD init.sh /init.sh # Set correct executable permissions RUN chmod +x /init.sh # Define default command CMD [\u0026amp;quot;/init.sh\u0026amp;quot;] The init.sh script\n#!/bin/sh # Create Default RabbitMQ setup ( sleep 10 ; \\ # Create users # rabbitmqctl add_user \u0026lt;username\u0026gt; \u0026lt;password\u0026gt; rabbitmqctl add_user test_user test_user ; \\ # Set user rights # rabbitmqctl set_user_tags \u0026lt;username\u0026gt; \u0026lt;tag\u0026gt; rabbitmqctl set_user_tags test_user administrator ; \\ # Create vhosts # rabbitmqctl add_vhost \u0026lt;vhostname\u0026gt; rabbitmqctl add_vhost dummy ; \\ # Set vhost permissions # rabbitmqctl set_permissions -p \u0026lt;vhostname\u0026gt; \u0026lt;username\u0026gt; \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; rabbitmqctl set_permissions -p dummy test_user \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; \u0026#34;.*\u0026#34; ; \\ ) \u0026amp; rabbitmq-server $@ Place both of these files in a directory and build your image:\n$ docker build -t my_rabbitmq_image . Start a container based on the image using:\n$ docker run --rm=true --name my_rabbitmq_container -p 5672:5672 -p 15672:15672 my_rabbitmq_image Then in your browser navigate to http://localhost:15672 and see if all is ok!\nNote: When using Boot2Docker make sure to replace the localhost with the correct IP.\n","date":"June 11, 2015","href":"https://mpas.github.io/posts/2015/06/11/20150611-docker-rabbitmq-default-users/","summary":"When creating an Docker image it is nice to have predefined users and vhosts without manually having to create them after the Docker RabbitMQ image has started.\nThe following is a Dockerfile that extends the default Docker RabbitMQ image including the Management Plugin and it creates a standard set of users / vhosts when the container is created from the image.\nIt involves a init.sh script that is used to create the users and vhosts.","tags":["docker","rabbitmq"],"title":"Setting up Docker RabbitMQ with predefined users/vhosts","type":"posts"},{"categories":"[]","content":"A HttpServletRequestWrapper may be handy if you want to be able to read the HTTP Body multi times after you consume it in a filter. The ServletInputStream 3.1 changed a bit and the following methods have to be implemented.\nisFinished isReady setReadListener import com.google.common.primitives.Bytes; import javax.servlet.ReadListener; import javax.servlet.ServletInputStream; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletRequestWrapper; import java.io.ByteArrayInputStream; import java.io.IOException; import java.io.InputStream; import java.util.Arrays; public class AuthenticationRequestWrapper extends HttpServletRequestWrapper { // tag::variables[] private byte[] requestBody = new byte[0]; private boolean bufferFilled = false; // end::variables[] /** - Constructs a request object wrapping the given request. * - @param request The request to wrap - @throws IllegalArgumentException if the request is null */ public AuthenticationRequestWrapper(HttpServletRequest request) { super(request); } // tag::getRequestBody[] public byte[] getRequestBody() throws IOException { if (bufferFilled) { return Arrays.copyOf(requestBody, requestBody.length); } InputStream inputStream = super.getInputStream(); byte[] buffer = new byte[102400]; // 100kb buffer int bytesRead; while ((bytesRead = inputStream.read(buffer)) != -1) { requestBody = Bytes.concat(this.requestBody, Arrays.copyOfRange(buffer, 0, bytesRead)); // \u0026lt;1\u0026gt; } bufferFilled = true; return requestBody; } // end::getRequestBody[] // tag::getInputStream[] @Override public ServletInputStream getInputStream() throws IOException { return new CustomServletInputStream(getRequestBody()); // \u0026lt;1\u0026gt; } // end::getInputStream[] private static class CustomServletInputStream extends ServletInputStream { private ByteArrayInputStream buffer; public CustomServletInputStream(byte[] contents) { this.buffer = new ByteArrayInputStream(contents); } @Override public int read() throws IOException { return buffer.read(); } @Override public boolean isFinished() { return buffer.available() == 0; } @Override public boolean isReady() { return true; } @Override public void setReadListener(ReadListener listener) { throw new RuntimeException(\u0026#34;Not implemented\u0026#34;); } } } ","date":"June 10, 2015","href":"https://mpas.github.io/posts/2015/06/10/20150610-httpservletwrapper-3.1/","summary":"A HttpServletRequestWrapper may be handy if you want to be able to read the HTTP Body multi times after you consume it in a filter. The ServletInputStream 3.1 changed a bit and the following methods have to be implemented.\nisFinished isReady setReadListener import com.google.common.primitives.Bytes; import javax.servlet.ReadListener; import javax.servlet.ServletInputStream; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletRequestWrapper; import java.io.ByteArrayInputStream; import java.io.IOException; import java.io.InputStream; import java.util.Arrays; public class AuthenticationRequestWrapper extends HttpServletRequestWrapper { // tag::variables[] private byte[] requestBody = new byte[0]; private boolean bufferFilled = false; // end::variables[] /** - Constructs a request object wrapping the given request.","tags":["java"],"title":"HTTPServletRequestWrapper for ServletInputStream 3.1","type":"posts"},{"categories":"[]","content":"The new Docker Registry (2.x) has just been released and is rewritten in Go. The default installation now requires SSL security and I was looking for a way to secure the Registry using a NGINX SSL proxy where users need to provide username/password to access the registry. The setup of the NGINX proxy can be done manually but i decided to see if i can reuse the excellent images from Container Solutions to ease the installation.\nSo the setup will be that we install the Docker Registry and proxy the SSL user access via self signed certificates using an NGINX proxy image provided by Container Solutions. Check here for more information\nInstallation of the remote docker registry will be done by using on an Amazon EC2 (Linux AMI). Currently the free tier Amazon Linux AMI 2015.03 (HVM), SSD Volume Type - ami-a10897d6. So spin up the Amazon AMI and let\u0026rsquo;s install Docker.\nNote: when you spin up your Amazon AMI make sure to remember the FQDN/DNS name! We need this name to generate the SSL certificates!\nExample: \u0026lt;domain-name\u0026gt; = ec2-52-16-247-220.eu-west-1.compute.amazonaws.com So spin up your AMI and install Docker!\nInstalling docker login into your Amazon AMI update the system and install Docker $ sudo yum update -y $ sudo wget -qO- https://get.docker.com/ | sh add the ec2-user to the docker group (optional) $ sudo usermod -aG docker ec2-user start Docker $ sudo service docker start make sure Docker can run the basic \u0026ldquo;hello-world\u0026rdquo; $ sudo docker run hello-world Create Docker Registry data and configuration directories We are going to store the registry image data inside /opt/docker/registry/data and configuration files such as the ssl certificates and user login inside /opt/docker/registry/conf.\ncreate data folders for Docker Registry data and configuration $ sudo mkdir -p /opt/docker/registry/data $ sudo mkdir -p /opt/docker/registry/conf Run the Docker Registry Now we are able to run the Docker Registry, the data for images that will be pushed are going to be stored in /opt/docker/registry/data and the container will be named docker-registry\nrun the registry and name it docker-registry $ sudo docker run -d -v /opt/docker/registry/data:/tmp/registry-dev \\ --name docker-registry registry:2.0.1 test if the registry is actually running $ sudo docker ps So now we have a running Docker Registry, but still no SSL proxy and no user accounts to get access to the registry.\nGenerate self signed certificates for our SSL proxy The result of the certificate generation will be placed in /opt/docker/registry/conf and named docker-registry.crt and docker-registry.key.\nNote: The docker-registry.crt file is important, we will need this later on to configure our local Docker client on the machine that is going to access the remote registry. So after generating the docker-registry.crt file, grab this and store it on your local machine in a place where you can find it.\ngenerate the certificates $ sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 \\ -keyout /opt/docker/registry/conf/docker-registry.key \\ -out /opt/docker/registry/conf/docker-registry.crt Accept all defaults and make sure you give the correct FQDN /DNS name = \u0026lt;domain-name\u0026gt;.\nCreate passwords for access to the Docker Registry In order to let users login into the registry we need to create users (user1/user2). This will be done by using htpasswd. The user data will be stored in docker-registry.htpasswd file and placed in the /opt/docker/registry/conf directory.\ninstall htpasswd $ sudo yum install httpd-tools -y create the users $ sudo htpasswd -c /opt/docker/registry/conf/docker-registry.htpasswd user1 $ sudo htpasswd /opt/docker/registry/conf/docker-registry.htpasswd user2 Note: when creating the second user omit the -c otherwise the docker-registry.htpasswd file will be get overwritten!\nRun the NGINX Proxy As mentioned we are going to use the image from Container Solutions to run our NGINX proxy.\nstart the NGINX proxy and name it docker-registry-proxy $ sudo docker run -d -p 443:443 \\ -e REGISTRY_HOST=\u0026#34;docker-registry\u0026#34; -e REGISTRY_PORT=\u0026#34;5000\u0026#34; -e SERVER_NAME=\u0026#34;localhost\u0026#34; \\ --link docker-registry:docker-registry \\ -v /opt/docker/registry/conf/docker-registry.htpasswd:/etc/nginx/.htpasswd:ro \\ -v /opt/docker/registry/conf:/etc/nginx/ssl:ro \\ --name docker-registry-proxy containersol/docker-registry-proxy After this step we have a running Docker Registry which is secured using Self Signed certificates and users are able to login using their username/password.\nTo test this navigate to your registry by using a browser (Chrome) and access: https://\u0026lt;domain-name\u0026gt;:443/v2/. After accepting the security warning provide a username/password and the output should be {}.\nConfigure the local Docker client Now that we have a running secured Docker Registry we can configure the Docker client on our machine that is going to access the remote Registry. For this we need a copy of the earlier docker-registry.crt file.\ncopy the docker-registry.crt file from our server to your local machine. This file is located in /opt/docker/registry/conf. Put the copy in a place where you can find it. Ubuntu Docker Client In order to get the local client working, we need to install Docker and register the docker-registry.crt certificate file!\ninstall docker $ sudo wget -qO- https://get.docker.com/ | sh $ sudo service docker start create a directory holding our extra certificates $ sudo mkdir /usr/share/ca-certificates/extra copy the docker-registry.crt file to the directory /usr/share/ca-certificates/extra\nregister the certificate\n$ sudo dpkg-reconfigure ca-certificates Now you are almost ready!\nrestart the local Docker client $ sudo service docker restart login onto your remote registry using $ docker login \u0026lt;domain-name\u0026gt;:port Now we have a remote Docker Registry and the Docker Client is able to connect!\n","date":"June 5, 2015","href":"https://mpas.github.io/posts/2015/06/05/20150605-docker-setup-registry/","summary":"The new Docker Registry (2.x) has just been released and is rewritten in Go. The default installation now requires SSL security and I was looking for a way to secure the Registry using a NGINX SSL proxy where users need to provide username/password to access the registry. The setup of the NGINX proxy can be done manually but i decided to see if i can reuse the excellent images from Container Solutions to ease the installation.","tags":["docker"],"title":"Installing Docker Registry 2.0.1 using self signed certificates","type":"posts"},{"categories":"[]","content":"When upgrading to Grails 2.4.2 i ran into an issue where the following error message would pop up.\nError creating bean with name \u0026#39;grailsApplication\u0026#39; defined in ServletContext resource [/WEB-INF/applicationContext.xml]: Cannot resolve reference to bean \u0026#39;grailsResourceLoader\u0026#39; while setting bean property \u0026#39;grailsResourceLoader\u0026#39;; To solve this issue you need to delete some lines in the \u0026lt;grails-app\u0026gt;/web-app/WEB-INF/applicationContext.xml file.\nDelete the following lines:\n\u0026lt;property name=\u0026#34;grailsResourceLoader\u0026#34; ref=\u0026#34;grailsResourceLoader\u0026#34; /\u0026gt; \u0026lt;bean id=\u0026#34;grailsResourceLoader\u0026#34; class=\u0026#34;org.codehaus.groovy.grails.commons.GrailsResourceLoaderFactoryBean\u0026#34; /\u0026gt; And you should be up and running quickly.\n","date":"July 4, 2014","href":"https://mpas.github.io/posts/2014/07/04/20140724-upgrade-grails-2.4.2/","summary":"When upgrading to Grails 2.4.2 i ran into an issue where the following error message would pop up.\nError creating bean with name \u0026#39;grailsApplication\u0026#39; defined in ServletContext resource [/WEB-INF/applicationContext.xml]: Cannot resolve reference to bean \u0026#39;grailsResourceLoader\u0026#39; while setting bean property \u0026#39;grailsResourceLoader\u0026#39;; To solve this issue you need to delete some lines in the \u0026lt;grails-app\u0026gt;/web-app/WEB-INF/applicationContext.xml file.\nDelete the following lines:\n\u0026lt;property name=\u0026#34;grailsResourceLoader\u0026#34; ref=\u0026#34;grailsResourceLoader\u0026#34; /\u0026gt; \u0026lt;bean id=\u0026#34;grailsResourceLoader\u0026#34; class=\u0026#34;org.codehaus.groovy.grails.commons.GrailsResourceLoaderFactoryBean\u0026#34; /\u0026gt; And you should be up and running quickly.","tags":["grails"],"title":"Upgrading from Grails 2.3.8 to 2.4.2","type":"posts"},{"categories":"[]","content":"When creating a layout it may occur that you want to skip rendering certain types of content, or render only specific content in a part of your layout.\nExample: You want to only render contenttype \u0026lsquo;post\u0026rsquo; use the following code in your template:\n{{ if eq .Type \u0026#34;post\u0026#34; }} {{ .Title }} {{ .Content }} {{ end }} ","date":"June 9, 2014","href":"https://mpas.github.io/posts/2014/06/09/20140609-hugo-skip-certain-content-to-be-rendered/","summary":"When creating a layout it may occur that you want to skip rendering certain types of content, or render only specific content in a part of your layout.\nExample: You want to only render contenttype \u0026lsquo;post\u0026rsquo; use the following code in your template:\n{{ if eq .Type \u0026#34;post\u0026#34; }} {{ .Title }} {{ .Content }} {{ end }} ","tags":["hugo"],"title":"Skip a contenttype/section to be renderend","type":"posts"},{"categories":"[]","content":"For those who have not yet got into contact with Vert.x, the book Instant Vert.x (54 pages in total of which 40 pages are “real” content) is a nice introduction to the underlying concepts.\nAs the name suggests, you can read the book in \u0026ldquo;an instant\u0026rdquo; and takes the reader through all high level concepts. The information in the book mostly stays at the concept level and provides some basic usage examples.\nFor those who have not yet had the opportunity to learn about Vert.x, I would not immediately recommend this book. The online documentation section on Vertx.io contains the same information. But if you like a book with information nicely put into digestible chapters then this book is a good fit.\nPersonally i hoped to get some more technical information and how-to information from this book but it is really targeted towards people that are just starting or have a beginning interest in Vert.x.\nOverall, I really liked the compactness and pace of the book. It is an easy read and you quickly gain knowledge on the high level concepts of Vert.x. While noted earlier you can get the information also on Vertx.io website or other places, it\u0026rsquo;s is nice to have all information aggregated in one place. This book is a good start in your journey to learn about Vert.x.\nGo to the Vert.x website Get the book at Pack Publishing ","date":"November 27, 2013","href":"https://mpas.github.io/posts/2013/11/27/20131127-bookreview-instant-vertx/","summary":"For those who have not yet got into contact with Vert.x, the book Instant Vert.x (54 pages in total of which 40 pages are “real” content) is a nice introduction to the underlying concepts.\nAs the name suggests, you can read the book in \u0026ldquo;an instant\u0026rdquo; and takes the reader through all high level concepts. The information in the book mostly stays at the concept level and provides some basic usage examples.","tags":["vertx","java","review"],"title":"Bookreview : Instant Vert.x","type":"posts"},{"categories":"[]","content":"When using Chrome (in Linux Mint) and pressing the backspace key nothing happens. You would expect to go back to the previous url you have visited. To enable the default behaviour you could install the following extension:\nhttps://chrome.google.com/webstore/detail/backspace-as-backforward/aeffggjddcchloadflonilaahpclmbnm?hl=en ","date":"May 30, 2013","href":"https://mpas.github.io/posts/2013/05/30/20130530-backspace-mint/","summary":"When using Chrome (in Linux Mint) and pressing the backspace key nothing happens. You would expect to go back to the previous url you have visited. To enable the default behaviour you could install the following extension:\nhttps://chrome.google.com/webstore/detail/backspace-as-backforward/aeffggjddcchloadflonilaahpclmbnm?hl=en ","tags":["linux","mint","chrome"],"title":"Use backspace key to go back in Google Chrome","type":"posts"},{"categories":"[]","content":"When automounting an NTFS volume under Linux (Mint) you can do this using the /etc/fstab file. dummy dummy dummy\n$ sudo pico /etc/fstab Add a line in the /etc/fstab file:\n# custom mount point /dev/sdb1 /media/windows-c ntfs-3g defaults 0 0 And take the mount into effect.\n$ mount -a ","date":"May 28, 2013","href":"https://mpas.github.io/posts/2013/05/28/20130528-automount-ntfs-mint/","summary":"When automounting an NTFS volume under Linux (Mint) you can do this using the /etc/fstab file. dummy dummy dummy\n$ sudo pico /etc/fstab Add a line in the /etc/fstab file:\n# custom mount point /dev/sdb1 /media/windows-c ntfs-3g defaults 0 0 And take the mount into effect.\n$ mount -a ","tags":["linux","mint"],"title":"Automount NTFS volume under Linux (Mint)","type":"posts"},{"categories":"[]","content":"Past week I really enjoyed visiting \u0026amp; speaking at the Grails Conference (GR8Conf 2013). The organisation made it possible that for me to host a talk about \u0026ldquo;Using Grails to power your Electric Car\u0026rdquo;.\nUsing Grails to power your Electric Car\n","date":"May 25, 2013","href":"https://mpas.github.io/posts/2013/05/25/20130525-grails-to-power-your-electric-car/","summary":"Past week I really enjoyed visiting \u0026amp; speaking at the Grails Conference (GR8Conf 2013). The organisation made it possible that for me to host a talk about \u0026ldquo;Using Grails to power your Electric Car\u0026rdquo;.\nUsing Grails to power your Electric Car","tags":["gr8conf","grails"],"title":"Using Grails to power your Electric Car","type":"posts"},{"categories":"[]","content":"When installing a fresh version of Chrome it may occur that all Bookmarks sync ok but the extensions not. The following solution worked for my system.\nOpen Google Chrome Goto \u0026ldquo;Settings\u0026rdquo; Open \u0026ldquo;Advanced Settings\u0026rdquo; Check \u0026ldquo;Developer mode\u0026rdquo; Press \u0026ldquo;Update extensions now\u0026rdquo; ","date":"May 24, 2013","href":"https://mpas.github.io/posts/2013/05/24/20130524-chrome-not-synchronizing-extensions/","summary":"When installing a fresh version of Chrome it may occur that all Bookmarks sync ok but the extensions not. The following solution worked for my system.\nOpen Google Chrome Goto \u0026ldquo;Settings\u0026rdquo; Open \u0026ldquo;Advanced Settings\u0026rdquo; Check \u0026ldquo;Developer mode\u0026rdquo; Press \u0026ldquo;Update extensions now\u0026rdquo; ","tags":["chrome"],"title":"Google Chrome not synchronizing extensions","type":"posts"},{"categories":"[]","content":"Sourcecode:\npackage helloworld; import org.vertx.java.core.Handler; import org.vertx.java.core.http.HttpServerRequest; import org.vertx.java.deploy.Verticle; public class Server extends Verticle { public void start() { vertx.createHttpServer().requestHandler(new Handler\u0026lt;HttpServerRequest\u0026gt;() { public void handle(HttpServerRequest req) { req.response.headers().put(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/html; charset-UTF-8\u0026#34;); req.response.end(\u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;Hello from vert.x!\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#34;); } }).listen(8080); } } ","date":"May 16, 2013","href":"https://mpas.github.io/posts/2013/05/16/20130516-simple-helloworld-verticle/","summary":"Sourcecode:\npackage helloworld; import org.vertx.java.core.Handler; import org.vertx.java.core.http.HttpServerRequest; import org.vertx.java.deploy.Verticle; public class Server extends Verticle { public void start() { vertx.createHttpServer().requestHandler(new Handler\u0026lt;HttpServerRequest\u0026gt;() { public void handle(HttpServerRequest req) { req.response.headers().put(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/html; charset-UTF-8\u0026#34;); req.response.end(\u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;Hello from vert.x!\u0026lt;/h1\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#34;); } }).listen(8080); } } ","tags":["java","vertx"],"title":"Simple Helloworld verticle","type":"posts"},{"categories":"[]","content":"Sometimes it may happen that the automatic migrations in a Grails project may come to a hold due to the fact that Liquibase keeps waiting for a changelog lock. At the end this will result in a application that is not going to be deployed.\n... Waiting for changelog lock.... Waiting for changelog lock.... Waiting for changelog lock.... ... To solve this take the following steps:\nStop the application container (example: Tomcat) In the database look for a table called DATABASECHANGELOGLOCK In the table there is a record with id=1, change the following values: locked -\u0026gt; 0 lockgranted -\u0026gt; null lockedby -\u0026gt; null After updating this record start the application container Notes:\nTo see who has locked the database (normally the local machine):\nselect * from DATABASECHANGELOGLOCK; To update the record update DATABASECHANGELOGLOCK set locked=0, lockgranted=null, lockedby=null where id=1 ","date":"February 28, 2013","href":"https://mpas.github.io/posts/2013/02/28/20130228-grails-migrations-and-waiting-for-changelog/","summary":"Sometimes it may happen that the automatic migrations in a Grails project may come to a hold due to the fact that Liquibase keeps waiting for a changelog lock. At the end this will result in a application that is not going to be deployed.\n... Waiting for changelog lock.... Waiting for changelog lock.... Waiting for changelog lock.... ... To solve this take the following steps:\nStop the application container (example: Tomcat) In the database look for a table called DATABASECHANGELOGLOCK In the table there is a record with id=1, change the following values: locked -\u0026gt; 0 lockgranted -\u0026gt; null lockedby -\u0026gt; null After updating this record start the application container Notes:","tags":["grails","migrations"],"title":"Grails migrations and 'Waiting for changelog lock'","type":"posts"},{"categories":"[]","content":"The Spock \u0026amp; Build Test Data plugins both are wonderful additions to your toolkit if you are creating tests without getting into the hassle of constantly building up your object graph. You can focus on what you want to test!\nWhen testing constraints on an object it can be used as follows:\nclass Foo { String name // name of foo Integer age // age of foo static constraints = { name nullable: false, blank: false // name may never be nullable or blank age nullable: true } } And the Spock test\n@Build(Foo) @TestFor(Foo) class FooSpec extends Specification { def \u0026#34;Name of Foo must exist\u0026#34;() { given: \u0026#34;setting up the constraints\u0026#34; mockForConstraintsTests(Foo) when: \u0026#34;creating a Foo\u0026#34; Foo foo = Foo.buildWithoutSave() then: \u0026#34;validation should trigger\u0026#34; assertFalse foo.validate() and: \u0026#34;validation error should be on the value field\u0026#34; assert foo.errors.allErrors.first().field == \u0026#34;name\u0026#34; } } ","date":"January 13, 2013","href":"https://mpas.github.io/posts/2013/01/13/20130113-testing-constraints-with-build-test-data/","summary":"The Spock \u0026amp; Build Test Data plugins both are wonderful additions to your toolkit if you are creating tests without getting into the hassle of constantly building up your object graph. You can focus on what you want to test!\nWhen testing constraints on an object it can be used as follows:\nclass Foo { String name // name of foo Integer age // age of foo static constraints = { name nullable: false, blank: false // name may never be nullable or blank age nullable: true } } And the Spock test","tags":["build","testdata","grails","spock","test"],"title":"Testing constraints with Build Test Data","type":"posts"},{"categories":"[]","content":"Ever asked yourself the question how you could get the Enum from a String value? This is particularly usefull when you use Enum values in your screens and pass back the value of the enum!\nEnum.valueOf(YourClassName.class, \u0026#34;String Value\u0026#34;) http://docs.oracle.com/javase/6/docs/api/java/lang/Enum.html\n","date":"January 3, 2013","href":"https://mpas.github.io/posts/2013/01/03/20130103-getting-enum-from-string-value/","summary":"Ever asked yourself the question how you could get the Enum from a String value? This is particularly usefull when you use Enum values in your screens and pass back the value of the enum!\nEnum.valueOf(YourClassName.class, \u0026#34;String Value\u0026#34;) http://docs.oracle.com/javase/6/docs/api/java/lang/Enum.html","tags":["enum","java"],"title":"Getting the Enum from a String value","type":"posts"},{"categories":"[]","content":"Recently i had the pleasure of installing and using vsftp on a Ubuntu Linux 12.04 system. I closely followed the instructions but keeped getting my head banging against a wall with an error message: 500 OOPS: vsftpd: refusing to run with writable root inside chroot()\nAfter some googling around i found the following fix.. chmod the folder that the ftp user comes in to as he first login (root folder) by using in terminal:\nsudo chmod a-w /home/user Create a subfolder within the folder, either by the use of GUI or if you only have terminal it\u0026rsquo;s:\nsudo mkdir /home/user/newfolder Now you should be able to log in and read write within the \u0026ldquo;newfolder\u0026rdquo;.\nYou will NOT be able to write in the root folder itself from the ftp with the chmod a-w, so that is the reason for the subfolder, there you can.\nhttp://askubuntu.com/questions/128180/vsftpd-stopped-working-after-update\n","date":"September 18, 2012","href":"https://mpas.github.io/posts/2012/09/18/20120918-troubles-getting-vsftpd-user-logged-in/","summary":"Recently i had the pleasure of installing and using vsftp on a Ubuntu Linux 12.04 system. I closely followed the instructions but keeped getting my head banging against a wall with an error message: 500 OOPS: vsftpd: refusing to run with writable root inside chroot()\nAfter some googling around i found the following fix.. chmod the folder that the ftp user comes in to as he first login (root folder) by using in terminal:","tags":["ftps","linux","ubuntu","vsftpd"],"title":"Troubles getting vsftpd user logged in?","type":"posts"},{"categories":"[]","content":"We recently changed the way how we load configuration files in a Grails project. Normally we to use the .properties file format, but this has some serious disadvantages.\nYou cannot deal with all Grails Mail settings in the configuration file You cannot use the log4j DSL to extract the logging configuration outside your application etc.. In our hunt for a good way to load configuration files we asked question on the mailinglist and also found this blogpost which was the start for our implementation of loading the external configuration files.\nWe modified some small things and added a way of loading a configuration file that is resident in the root of a Grails project. So when developing with IntelliJ for example the config file is at your fingertips in the root of the application project structure. We must also note that we are very happy with the fact that the Grails community was more then helpfull in helping us out here!\n// -------------------------------------------------------------------------------- // // - START: CONFIGURATION FILE LOADING -------------------------------------------- // // -------------------------------------------------------------------------------- // // locations to search for config files that get merged into the main config // config files can either be Java properties files or ConfigSlurper scripts def ENV_NAME = \u0026#34;${appName}.config.location\u0026#34; if(!grails.config.locations || !(grails.config.locations instanceof List)) { grails.config.locations = [] } println \u0026#34;--------------------------------------------------------------------------------\u0026#34; println \u0026#34;- Loading configuration file -\u0026#34; println \u0026#34;--------------------------------------------------------------------------------\u0026#34; // 1: check for environment variable that has been set! This variable must point to the // configuration file that must be used. Can be a .groovy or .properties file! if(System.getenv(ENV_NAME) \u0026amp;\u0026amp; new File(System.getenv(ENV_NAME)).exists()) { println(\u0026#34;Including System Environment configuration file: \u0026#34; + System.getenv(ENV_NAME)) grails.config.locations \u0026lt;\u0026lt; \u0026#34;file:\u0026#34; + System.getenv(ENV_NAME) // 2: check for commandline properties! // Use it like (examples): // grails -D[name of app].config.location=/tmp/[name of config file].groovy run-app // or // grails -D[name of app].config.location=/tmp/[name of config file].properties run-app // } else if(System.getProperty(ENV_NAME) \u0026amp;\u0026amp; new File(System.getProperty(ENV_NAME)).exists()) { println \u0026#34;Including configuration file specified on command line: \u0026#34; + System.getProperty(ENV_NAME) grails.config.locations \u0026lt;\u0026lt; \u0026#34;file:\u0026#34; + System.getProperty(ENV_NAME) // 3: check on local project config file in the project root directory } else if (new File(\u0026#34;./${appName}-config.groovy\u0026#34;).exists()) { println \u0026#34;*** User defined config: file:./${appName}-config.groovy ***\u0026#34; grails.config.locations = [\u0026#34;file:./${appName}-config.groovy\u0026#34;] } else if (new File(\u0026#34;./${appName}-config.properties\u0026#34;).exists()) { println \u0026#34;*** User defined config: file:./${appName}-config.properties ***\u0026#34; grails.config.locations = [\u0026#34;file:./${appName}-config.groovy\u0026#34;] // 4: check on local project config file in ${userHome}/.grails/... } else if (new File(\u0026#34;${userHome}/.grails/${appName}-config.groovy\u0026#34;).exists()) { println \u0026#34;*** User defined config: file:${userHome}/.grails/${appName}-config.groovy ***\u0026#34; grails.config.locations = [\u0026#34;file:${userHome}/.grails/${appName}-config.groovy\u0026#34;] } else if (new File(\u0026#34;${userHome}/.grails/${appName}-config.properties\u0026#34;).exists()) { println \u0026#34;*** User defined config: file:${userHome}/.grails/${appName}-config.properties ***\u0026#34; grails.config.locations = [\u0026#34;file:${userHome}/.grails/${appName}-config.properties\u0026#34;] // 5: we have problem!! } else { println \u0026#34;********************************************************************************\u0026#34; println \u0026#34;* No external configuration file defined *\u0026#34; println \u0026#34;********************************************************************************\u0026#34; } println \u0026#34;(*) grails.config.locations = ${grails.config.locations}\u0026#34; println \u0026#34;--------------------------------------------------------------------------------\u0026#34; // -------------------------------------------------------------------------------- // // - END: CONFIGURATION FILE LOADING ---------------------------------------------- // // -------------------------------------------------------------------------------- // ","date":"September 17, 2012","href":"https://mpas.github.io/posts/2012/09/17/20120917-grails-load-external-configuration-files-update/","summary":"We recently changed the way how we load configuration files in a Grails project. Normally we to use the .properties file format, but this has some serious disadvantages.\nYou cannot deal with all Grails Mail settings in the configuration file You cannot use the log4j DSL to extract the logging configuration outside your application etc.. In our hunt for a good way to load configuration files we asked question on the mailinglist and also found this blogpost which was the start for our implementation of loading the external configuration files.","tags":["grails","configuration"],"title":"Loading Grails configuration files update!","type":"posts"},{"categories":"[]","content":"The use of \u0026lsquo;Config.groovy\u0026rsquo; as a placeholder for configuration settings is nice, but not always sufficient. The \u0026lsquo;Config.groovy\u0026rsquo; file will get compiled and packaged inside the WAR file you are creating. If you want to externalize the configuration and have a need to configure settings outside the deployed (WAR file) application you can use property files (.properties) to achieve that.\nA simple mechanism to load these property files is to place a short snippet of code in the \u0026lsquo;Config.groovy\u0026rsquo; that will load a specific configuration file from the filesystem, depending on the availability.\ngrails.config.locations = [\u0026#34;classpath:application-config.properties\u0026#34;, \u0026#34;file:./application-config.properties\u0026#34;] This snippet will first try to load the property file from the classpath and if that fails you have a backup on the filesystem. This opens opportunities to load a different property file during development! When you deploy the application you can place the \u0026lsquo;application-config.properties\u0026rsquo; file inside a folder which is available in the classpath. For Apache Tomcat this would be the \u0026rsquo;lib\u0026rsquo; folder!\nThis gives the opportunity to configure the application outside the \u0026lsquo;Config.groovy\u0026rsquo; file so any changes made the the property file will be reflected in your environment.\n","date":"September 16, 2012","href":"https://mpas.github.io/posts/2012/09/16/20120916-grails-load-external-configuration-files/","summary":"The use of \u0026lsquo;Config.groovy\u0026rsquo; as a placeholder for configuration settings is nice, but not always sufficient. The \u0026lsquo;Config.groovy\u0026rsquo; file will get compiled and packaged inside the WAR file you are creating. If you want to externalize the configuration and have a need to configure settings outside the deployed (WAR file) application you can use property files (.properties) to achieve that.\nA simple mechanism to load these property files is to place a short snippet of code in the \u0026lsquo;Config.","tags":["grails","configuration"],"title":"Loading external Configuration files in a Grails application","type":"posts"},{"categories":"[]","content":"When there is a need to work with images (thumbnailing, watermark, resize etc.) there is always ImageMagick that comes to the rescue. Combining this image utility powerhouse with the Grails framework is a task which can be easily accomplished.\nSteps:\nInstall ImageMagick according to the installation instructions. It contains a utility called convert which we will need later on! This utility takes care of the conversion of images to thumbnails, watermarks etc. So remember where this utility is installed on your system! Make sure that ImageMagick is installed correctly be converting an image to a thumbnail by using the following command in a terminal. /opt/local/bin/convert \u0026lt;filename\u0026gt; -thumbnail 70x70 \u0026lt;thumbnail-filename\u0026gt; example:\n/opt/local/bin/convert /tmp/image-001.jpg -thumbnail 70x70 /tmp/thumbnail-image-001.jpg Create some code that calls the ImageMagick convert utility with the correct parameters to enable you to achieve what you want. Something like below:\ndef createThumbnail(File file) { def command = \u0026#34;/opt/local/bin/convert ${file.canonicalPath} \u0026#34; + \u0026#34;-thumbnail 70x70 \u0026#34; + \u0026#34;/images/thumbs\u0026#34; + File.separator + \u0026#34;${file.name}\u0026#34; def proc = Runtime.getRuntime().exec(command) int exitStatus; while (true) { try { exitStatus = proc.waitFor(); break; } catch (java.lang.InterruptedException e) { log.debug(\u0026#34;Creating thumbnail - Interrupted: Ignoring and waiting\u0026#34;) } } if (exitStatus != 0) { log.error(\u0026#34;Error executing command: exitStatus=[${exitStatus}]\u0026#34;) } log.debug(\u0026#34;Succesfully created thumbnail\u0026#34;) return (exitStatus == 0); } The above should give you some idea on how you could integrate Grails and ImageMagick into your own application.\n","date":"September 15, 2012","href":"https://mpas.github.io/posts/2012/09/15/20120915-grails-and-imagemagick/","summary":"When there is a need to work with images (thumbnailing, watermark, resize etc.) there is always ImageMagick that comes to the rescue. Combining this image utility powerhouse with the Grails framework is a task which can be easily accomplished.\nSteps:\nInstall ImageMagick according to the installation instructions. It contains a utility called convert which we will need later on! This utility takes care of the conversion of images to thumbnails, watermarks etc.","tags":["grails","imagemagick"],"title":"Combining ImageMagick and Grails","type":"posts"},{"categories":"[]","content":"No need to discuss that Vim is truly a great text editor. Wealth of features, great speed and extensive support for plugins. The installation of plugins is very easy. If you want to learn how to install plugins, make sure to check out the wiki.\nPathogen When you instal a plugin one may copy the files to the plugin directory. In a later stage you also want to delete a plugin and then the hunt for files starts. You need to track down which files belong to the specific plugin you want to delete. Pathogen to the rescue :)\nPathogen enables you to create sub-folders inside a bundle-folder which will acts a place holder for all your plugins nicely separated in a ‘folder per plugin’ structure. So if you need to delete a plugin then you just delete the correct plugin-folder and everything is gone.\nNormal installation of a Vim script is standard, you create a sub-folder below the bundle-folder, copy the Vim script and all is ok. BUT when you want to use a vimball then you need to do some additional steps.\nCreate a folder in which you want to later on extract the vimball. Preferably below the ‘bundle’ folder. mkdir ~/.vim/bundle/align ```console * Open the vimball with command ‘:e \u0026#39;location of your vimball’/‘name of your vimball\u0026#39; ```console :e ~/Downloads/Align.vba Tell Vim to use the vimball by issuing command ‘:UseVimball \u0026rsquo;location to extract’' :UseVimball ~/.vim/bundle/align Restart your Vim and your plugin should be available. ","date":"September 14, 2012","href":"https://mpas.github.io/posts/2012/09/14/20120914-installing-vimball-plugins-using-pathogen/","summary":"No need to discuss that Vim is truly a great text editor. Wealth of features, great speed and extensive support for plugins. The installation of plugins is very easy. If you want to learn how to install plugins, make sure to check out the wiki.\nPathogen When you instal a plugin one may copy the files to the plugin directory. In a later stage you also want to delete a plugin and then the hunt for files starts.","tags":["vim","pathogen"],"title":"Installing Vimball plugins when using Pathogen","type":"posts"},{"categories":"[]","content":"Back again to one of my favorites which is called Markdown. Once every now and then i forget how easy it is. Normally i use Textmate to do all my writing, but recently i have picked up VIM to do some editing etc. Why i did chose VIM? I will not trouble you with that decision :)\nUsing Textmate everything is easy, but when you want to use Markdown inside VIM it is somewhat different. But anything is different when using VIM :)\nSteps download Markdown from - The home of Markdown, It’s usual place as this is a Perl script you need to put it somewhere so OSX is able to execute it. start your terminal and create a directory inside usr/local/bin extract the downloaded file and put the Markdown.pl file * inside the user/local/bin directory inside the terminal chmod the Markdown.pl to 777 using the Installing Markdown as OSX Service creates a service to use Markdown You are done… :) ","date":"September 13, 2012","href":"https://mpas.github.io/posts/2012/09/13/20120913-installing-markdown-on-osx-use-it-inside-vim/","summary":"Back again to one of my favorites which is called Markdown. Once every now and then i forget how easy it is. Normally i use Textmate to do all my writing, but recently i have picked up VIM to do some editing etc. Why i did chose VIM? I will not trouble you with that decision :)\nUsing Textmate everything is easy, but when you want to use Markdown inside VIM it is somewhat different.","tags":["vim","markdown","osx"],"title":"Installing Markdown on OSX and use it inside VIM","type":"posts"},{"categories":"[]","content":"Recently i had the opportunity to show an exciting crowd a presentation about Groovy \u0026amp; Grails.\nThis happend during the Devnology Community Event at Baarn. If you want to view the presentation \u0026ndash;\u0026gt; check it out here!\n","date":"September 12, 2012","href":"https://mpas.github.io/posts/2012/09/12/20120912-introduction-groovy-and-grails/","summary":"Recently i had the opportunity to show an exciting crowd a presentation about Groovy \u0026amp; Grails.\nThis happend during the Devnology Community Event at Baarn. If you want to view the presentation \u0026ndash;\u0026gt; check it out here!","tags":["grails","presentation"],"title":"Introduction to Groovy \u0026 Grails","type":"posts"},{"categories":"[]","content":"You can set the default homepage for a Grails application by modifying the grails-app/conf/UrlMappings.groovy file. In a new Grails application this file will look like\nclass UrlMappings { static mappings = { \u0026#34;/$controller/$action?/$id?\u0026#34;{ constraints { // apply constraints here } } \u0026#34;/\u0026#34;(view:\u0026#34;/index\u0026#34;) \u0026#34;500\u0026#34;(view:\u0026#39;/error\u0026#39;) } } Replace the line:\n\u0026#34;/\u0026#34;(view:\u0026#34;/index\u0026#34;) with:\n\u0026#34;/\u0026#34;(controller:\u0026#39;home\u0026#39;, action:\u0026#34;/index\u0026#34;) This will result in the fact that when you start your Grails application and you enter the URL for your application it will trigger the HomeController and corresponding index action related to that controller.\n","date":"September 11, 2012","href":"https://mpas.github.io/posts/2012/09/11/20120911-grails-change-default-homepage/","summary":"You can set the default homepage for a Grails application by modifying the grails-app/conf/UrlMappings.groovy file. In a new Grails application this file will look like\nclass UrlMappings { static mappings = { \u0026#34;/$controller/$action?/$id?\u0026#34;{ constraints { // apply constraints here } } \u0026#34;/\u0026#34;(view:\u0026#34;/index\u0026#34;) \u0026#34;500\u0026#34;(view:\u0026#39;/error\u0026#39;) } } Replace the line:\n\u0026#34;/\u0026#34;(view:\u0026#34;/index\u0026#34;) with:\n\u0026#34;/\u0026#34;(controller:\u0026#39;home\u0026#39;, action:\u0026#34;/index\u0026#34;) This will result in the fact that when you start your Grails application and you enter the URL for your application it will trigger the HomeController and corresponding index action related to that controller.","tags":["grails"],"title":"Change default homepage for a Grails application","type":"posts"},{"categories":"[]","content":"So assume you are assigned to a JEE/Web project with no written functional requirements, no technical design, no functional and unit tests and even no business process description. Sounds really hopeless, but it is your responsibility to learn the system and make adjustments to it. Does this sound familiar?? Hopefully not :) But every now and then this scenario seems to happen.\nOne can start to complain :), stop working on the project or even better master the concept of Software Archeology. An additional thing is to adopt the use of Cobertura a code coverage tool which can easily be used to track down Functional Code Coverage. Normally the concept of Code Coverage is used to identify what code is executed during development and test phase. This to give an indication on how much code you cover with your testing strategy (often unit testing). This is IMHO something you will always want to know! But in the case you do not have unit tests or creating them is impossible due to the technical/organisational nature of the project, you can rely on creating functional tests and still track down the \u0026lsquo;functional\u0026rsquo; coverage with tools like Cobertura (or alternatives like Emma).\nThis tackles several problems:\nYou are creating functional tests which can be used for regression testing You are creating awareness on how little is tested or is known about the system Note: By functional testing we mean that we are going to test via the Web layer of the JEE project To see an example on how the reporting looks like, check out this sample report!\nHow to get Code Coverage information General process:\nCompile your software Instrument the compiled code Deploy your instrumented code and start the application Use the application or run automated functional tests Shutdown the application Generate your Code Coverage reports no step 7! All done :) The remaining part of this article is going to describe how you can get Functional Code Coverage information in the process of continously building, deploying and testing your software. Some elements are not explained due to the fact that other extensive information is given somewhere else on the web!\nCompile, Instrument and Build The Maven project enables you to build your software with great ease. Giving a few simple commands makes it able to build a project, deploy it and even integrate it with tools and technologies such as Cobertura. There is a even a Cobertura Maven Plugin to easily use Cobertura in a Maven build. We need to use Cobertura in the build phase because it will instrument the compiled code and generate a small file called \u0026lsquo;cobertura.ser\u0026rsquo; which is used as a kind of database that stores each call to a piece of code. The instrumented code and the database file are crucial because they contains all information needed to generate code coverage reports later on.\nRun and Test After the code is succesfully instrumented you may deploy the build artifacts together with the \u0026lsquo;cobertura.ser\u0026rsquo; file inside a JBoss JEE container and run your application.\nNote: In our project we used JBoss but offcourse you can use other application servers!\nThe JMeter project delivers an excellent tooling and technology which enables you to record your functional flow and lets you replay a scenario which was recorded earlier on. For more information on JMeter recording and usage, please check the JMeter project. But for now lets assume you have created a couple of functional tests, so you can execute them.\nGenerate coverage reports After the functional tests have been executed, the modified database file \u0026lsquo;cobertura.ser\u0026rsquo; can be collected and reports can be generated. Cobertura has some nice predefined reporting templates. After these stepes you should have inisight on what code is actually executed during a functional flow and this may contribute to your understanding of the application.\nNot once but do it always! The process of compiling, instrumenting, deploying, testing and reporting can be fully automatized. The famous Hudson comes to the rescue! When corectly implemented Hudson will serve you all information that you need on the moments you need it!\nTools \u0026amp; Technologies The folowing list provides information on the tools that are used:\nMaven -\u0026gt; used info for compiling and instrumenting your code (alternative to Ant) Cobertura -\u0026gt; used to get Code Coverage information JBoss -\u0026gt; used for running a JEE project JMeter -\u0026gt; used to record and playback functional tests Hudson -\u0026gt; used to automatically build \u0026amp; test your software ","date":"September 10, 2012","href":"https://mpas.github.io/posts/2012/09/10/20120910-functional-code-coverage-using-cobertura/","summary":"So assume you are assigned to a JEE/Web project with no written functional requirements, no technical design, no functional and unit tests and even no business process description. Sounds really hopeless, but it is your responsibility to learn the system and make adjustments to it. Does this sound familiar?? Hopefully not :) But every now and then this scenario seems to happen.\nOne can start to complain :), stop working on the project or even better master the concept of Software Archeology.","tags":["coverage","testing","cobertura","continous integration","hudson","jenkins","ssh"],"title":"Funcional Code Coverage using Cobertura","type":"posts"},{"categories":"[]","content":"Have you already implemented an multi-server artifact deployment using a Continuous Integration Engine? If not, then read ahead and maybe this article is of help.\nThe need for Continuous Integration A good practice in a software development methodology and lifecycle is the use of a Continuous Integration Engine. The adoption of Continuous Integration improves you software quality by quickly reporting failed builds so you can modify/correct your code. Popular Continuous Integration Engines can often be extended with software quality tooling so you can report on specific quality aspect of your software.\nThus informing developers and even other people who take an interest in the status of the latest build. IMHO a failed build can also be identified as code that compiles but that does not meet the quality standards set by your organization. You are off-course totally free in defining what in your opinion a failed build actually means!\nA good build compiles, quality requirements have been met and automatic functional and unit testing has been successful.\nThere a few popular Continuous Integration Engines available:\nHudson Extensible Continuous Integration Server CruiseControl Bamboo Probably there a some more, but for me needs the [Hudson Extensible Continuous Integration Server] works perfect.\nMy needs My Continuous Integration Engine:\nMust support multiple programming languages Java/.Net/Ruby Runs on multiple operating systems (Windows/Mac/Linux) Pluggable in the sense that there must be integration with for example JUnit, JMeter, Cobertura, Checkstyle etc. Must be able to send out notifications using email, twitter, Instant Messaging Seemless integration with CVS/SubVersion and GIT Simple and easy Configuration Must support timed builds \u0026amp; trigger builds from SCM commits Maintain a link between modified code and and some more.. All of these requirements and more have been succesfully fulfilled by using Hudson\nHudson and automatic deployment In every project a recurring problem arises, artifacts of a software build have to be distributed accross different servers and environments. How are the build artifacts going to be distributed and deployed? Why not let the Hudson server give a helping hand!!\nDeployment Scenario In the following scenario we will be distributing build artifacts from a central Hudson server to three different JBoss application servers al running Windows 2003 Server as Operating System. Next to the Hudson server we have a Subversion system which is used for SCM purposes.\nDistribution of build artifacts After a succesfull build, the artifacts have to be distributed to remote machines. Offcourse the latest code has been checked out from the SVN repository, compiled and tested. The distribution of the artifacts can be done in various ways. One can use Ftp, shared folders, SCM checkin etc..\nFor me the most easiest way to distribute build artifacts is using Secure Shell access aka SSH. This is a secure and a standardized manner for distribution. Lets assume we have the build artifacts somewhere on our Hudson server, we need a way of transfering them using SSH to a remote machine.\nTo accomplish this we need SSH access to the remote machine. With the help off CopSSH installing SSH is a breeze!\nPrepare for installation of SSH Prepare yourself by downloading:\nCopSSH – SSH service for Windows Putty – SSH client (used for connection to the SSH service) PSCP – SSH client for file transfer Plink – SSH client for executing remote commands Installing and enable remote access using SSH Install CopSSH on the remote system On the remote system enable a user for SSH access, see the installation guide of CopSSH Start Putty on your local machine Using Putty connect to the remote system and exchange security credentials You are now officially ready to remotely access the system using SSH If you want to enable the Hudson server to access the remote system, start Putty on the Hudson server and repeat step 4!\nNote: Make sure that your Hudson uses the same credentials then the account in which you exchange security credentials, otherwise remote access from Hudson server to the remote system will not work!\nExecute remote commands and Exchange files using SSH If you can succesfully access the remote server using Putty, it is time to exchange files or execute remote commands. This can be done by using 2 small commandline utilities called PSCP for file transfer and Plink for executing remote commands such as remotely deleting files etc.\nMake sure these are in you PATH settings so you can execute them everywhere!\nExamples for executing a remote command (substitute the %parameters% with your own ones)\n#create a directory plink -batch -pw %PASSWORD% %USERNAME%@%HOSTNAME% mkdir C:/tmp #delete a directory plink -batch -pw %PASSWORD% %USERNAME%@%HOSTNAME% rm -rf C:/tmp #stop a windows service plink -batch -pw %PASSWORD% %USERNAME%@%HOSTNAME% net stop %SERVICENAME% #upload a file pscp -pw %PASSWORD% %SOURCE% %USERNAME%@%HOSTNAME%:%DESTINATION% #upload multiple files pscp -pw %PASSWORD% %SOURCE%\\*.* %USERNAME%@%HOSTNAME%:%DESTINATION% The return of the .bat file So far we have enabled remote access using SSH / CopSSH, executed remote commands and transferred files. All the needed ingredients are in place to enable our Hudson server to remotely deploy build artifacts. In the job configuration of Hudson you can trigger a batch file after a succesfull build, so whenever a succesfull build occurs trigger a batch that executes a few commands to quickly deploy build artifacts to any number of remote servers.\nIn our case all deployment artifacts are copied to a central directory per project. So if we need to deploy a build, we can copy parts or the whole directory contents to a remote server.\nTo give an example see the following batch files: Main example for a batchfile that triggers stopping of the remote Windows Services gives the instruction on which files need to be remotely deployed and start the services again.\n-\u0026gt; filename = upload-project-to-development.bat @CLS @ECHO OFF SET USERNAME=%1 SET PASSWORD=%2 SET HOSTNAME=%3 SET JBOSSDIR=%4 SET SOURCEDIR=%5 @ECHO : - start upload procedure @ECHO : -- stopping servers @plink -batch -pw %PASSWORD% %USERNAME%@%HOSTNAME% net stop JBoss @ECHO : -- uploading files CALL upload-files.bat %USERNAME% %PASSWORD% %HOSTNAME% %JBOSSDIR% %SOURCEDIR%\\*.* @ECHO : -- starting servers @plink -batch -pw %PASSWORD% %USERNAME%@%HOSTNAME% net start JBoss @ECHO : - finished upload procedure The above script can be called easily by the Hudson server after a succesfull deployment.\nexample:\nupload-project-to-development scott tiger 10.0.0.100 d:/java/server/jboss-v5.0 d:/build_artifacts/projectx The example above:\nStops the JBoss server on the 10.0.0.100 host Passes some parameters to a file called “upload-files.bat” script Starts the JBoss servers again The script that executes the actual maintenance and uploads is the “upload-files.bat file”. All parameters are passed in by the calling script.\n-\u0026gt; filename = upload-files.bat @ECHO OFF SET USERNAME=%1 SET PASSWORD=%2 SET HOSTNAME=%3 SET JBOSSDIR=%4 SET SOURCE=%5 SET DESTINATION=%JBOSSDIR%/deploy @ECHO : - %HOSTNAME% - starting file copy @ECHO : -- %HOSTNAME% - deleting JBOSS tmp @plink -batch -pw %PASSWORD% %USERNAME%@%HOSTNAME% rm -rf %JBOSSDIR%/tmp @plink -batch -pw %PASSWORD% %USERNAME%@%HOSTNAME% mkdir %JBOSSDIR%/tmp @ECHO : -- %HOSTNAME% - deleting JBOSS work @plink -batch -pw %PASSWORD% %USERNAME%@%HOSTNAME% rm -rf %JBOSSDIR%/work @plink -batch -pw %PASSWORD% %USERNAME%@%HOSTNAME% mkdir %JBOSSDIR%/work @ECHO : -- %HOSTNAME% - deleting previous ears + jars @plink -batch -pw %PASSWORD% %USERNAME%@%HOSTNAME% rm %JBOSSDIR%/deploy/*.ear @plink -batch -pw %PASSWORD% %USERNAME%@%HOSTNAME% rm %JBOSSDIR%/deploy/*.jar @ECHO : -- %HOSTNAME% - copy remote files @pscp -pw %PASSWORD% %SOURCE% %USERNAME%@%HOSTNAME%:%DESTINATION% @ECHO : - %HOSTNAME% - finishing file copy The example above:\nRemoves the JBoss tmp \u0026amp; work directory Removes artifacts from previous builds Copies the artifacts to the remote JBoss deploy directory Steps taken So the list of tasks executed by calling the batch files with the correct parameters are:\nStopping the remote JBoss server Removing the remote JBoss tmp \u0026amp; work directories Removing the remote JBoss artifacts from previous deployments Copy files to the remote JBoss server Starting the JBoss server again From build to deployment So with a quick installation of SSH/Putty/Plink/PSCP we now have a modular and easy way of distributing files to remote systems. Offcourse there are lots of improvements to make, but for now it works without any problems!\nThe given examples can be easily modified so that after a succesfull build the artifact deployment to all of your servers can be done in a very simple and easy way. Notes\n","date":"September 9, 2012","href":"https://mpas.github.io/posts/2012/09/09/20120909-distributed-deployment-with-hudson-and-ssh/","summary":"Have you already implemented an multi-server artifact deployment using a Continuous Integration Engine? If not, then read ahead and maybe this article is of help.\nThe need for Continuous Integration A good practice in a software development methodology and lifecycle is the use of a Continuous Integration Engine. The adoption of Continuous Integration improves you software quality by quickly reporting failed builds so you can modify/correct your code. Popular Continuous Integration Engines can often be extended with software quality tooling so you can report on specific quality aspect of your software.","tags":["deployment","continous integration","hudson","ssh"],"title":"Distributed Deployment with Hudson \u0026 SSH","type":"posts"},{"categories":"[]","content":"A Grails application by default uses a in-memory HSQL database. To switch to a MySQL database the steps are simple and straightforward.\nDownload the MySQL JDBC driver [called a connector] from the MySQL website Extract the zip or tar archive Copy the driver (at this time of writing called mysql-connector-java-5.1.13-bin.jar into the grails-app/lib directory Configure your application datasource in file grails-app/conf/DataSource.groovy development { dataSource { dbCreate = \u0026#34;create-drop\u0026#34; // one of \u0026#39;create\u0026#39;, \u0026#39;create-drop\u0026#39;,\u0026#39;update\u0026#39; url = \u0026#34;jdbc:mysql://localhost:\u0026lt;port\u0026gt;/\u0026lt;database\u0026gt;\u0026#34; driverClassName = \u0026#34;com.mysql.jdbc.Driver\u0026#34; port = // default 3306 username = \u0026#34;\u0026lt;username\u0026gt;\u0026#34; password = \u0026#34;\u0026lt;password\u0026gt;\u0026#34; } } ","date":"September 5, 2012","href":"https://mpas.github.io/posts/2012/09/05/20120905-grails-replace-in-memory-db-with-mysql/","summary":"A Grails application by default uses a in-memory HSQL database. To switch to a MySQL database the steps are simple and straightforward.\nDownload the MySQL JDBC driver [called a connector] from the MySQL website Extract the zip or tar archive Copy the driver (at this time of writing called mysql-connector-java-5.1.13-bin.jar into the grails-app/lib directory Configure your application datasource in file grails-app/conf/DataSource.groovy development { dataSource { dbCreate = \u0026#34;create-drop\u0026#34; // one of \u0026#39;create\u0026#39;, \u0026#39;create-drop\u0026#39;,\u0026#39;update\u0026#39; url = \u0026#34;jdbc:mysql://localhost:\u0026lt;port\u0026gt;/\u0026lt;database\u0026gt;\u0026#34; driverClassName = \u0026#34;com.","tags":["grails","mysql"],"title":"Using MySQL instead of in-memory database for a Grails application","type":"posts"}]